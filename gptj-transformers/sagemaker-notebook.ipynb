{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "02e90d5c",
   "metadata": {},
   "source": [
    "# Optimized & deploy GPT-J on AWS inferentia2 with Amazon SageMaker\n",
    "\n",
    "In this end-to-end tutorial, you will learn how to speed up BERT inference down to `1ms` latency for text classification with Hugging Face Transformers, Amazon SageMaker, and AWS Inferentia2.\n",
    "\n",
    "You will learn how to: \n",
    "\n",
    "1. Convert BERT to AWS Neuron (Inferentia2) with `optimum-neuron`\n",
    "2. Create a custom `inference.py` script for `text-classification`\n",
    "3. Upload the neuron model and inference script to Amazon S3\n",
    "4. Deploy a Real-time Inference Endpoint on Amazon SageMaker\n",
    "5. Run and evaluate Inference performance of BERT on Inferentia2\n",
    "\n",
    "Let's get started! ðŸš€\n",
    "\n",
    "---\n",
    "\n",
    "*If you are going to use Sagemaker in a local environment (not SageMaker Studio or Notebook Instances). You need access to an IAM Role with the required permissions for Sagemaker. You can findÂ [here](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html)Â more about it.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "314fd00a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/aws-neuron/transformers-neuronx.git\n",
      "  Cloning https://github.com/aws-neuron/transformers-neuronx.git to /tmp/pip-req-build-tebky619\n",
      "  Running command git clone -q https://github.com/aws-neuron/transformers-neuronx.git /tmp/pip-req-build-tebky619\n",
      "Requirement already satisfied, skipping upgrade: accelerate in /home/ubuntu/.local/lib/python3.8/site-packages (from transformers-neuronx==0.4.20230629) (0.20.3)\n",
      "Requirement already satisfied, skipping upgrade: torch-neuronx in /usr/local/lib/python3.8/dist-packages (from transformers-neuronx==0.4.20230629) (1.13.0.1.6.1)\n",
      "Requirement already satisfied, skipping upgrade: transformers in /usr/local/lib/python3.8/dist-packages (from transformers-neuronx==0.4.20230629) (4.28.1)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from accelerate->transformers-neuronx==0.4.20230629) (1.21.6)\n",
      "Requirement already satisfied, skipping upgrade: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from accelerate->transformers-neuronx==0.4.20230629) (23.1)\n",
      "Requirement already satisfied, skipping upgrade: psutil in /usr/local/lib/python3.8/dist-packages (from accelerate->transformers-neuronx==0.4.20230629) (5.9.5)\n",
      "Requirement already satisfied, skipping upgrade: pyyaml in /home/ubuntu/.local/lib/python3.8/site-packages (from accelerate->transformers-neuronx==0.4.20230629) (6.0)\n",
      "Requirement already satisfied, skipping upgrade: torch>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from accelerate->transformers-neuronx==0.4.20230629) (1.13.1)\n",
      "Requirement already satisfied, skipping upgrade: torch-xla==1.13.0+torchneuron5 in /usr/local/lib/python3.8/dist-packages (from torch-neuronx->transformers-neuronx==0.4.20230629) (1.13.0+torchneuron5)\n",
      "Requirement already satisfied, skipping upgrade: libneuronxla==0.5.205 in /usr/local/lib/python3.8/dist-packages (from torch-neuronx->transformers-neuronx==0.4.20230629) (0.5.205)\n",
      "Requirement already satisfied, skipping upgrade: protobuf<5 in /usr/local/lib/python3.8/dist-packages (from torch-neuronx->transformers-neuronx==0.4.20230629) (3.20.2)\n",
      "Requirement already satisfied, skipping upgrade: filelock in /usr/lib/python3/dist-packages (from transformers->transformers-neuronx==0.4.20230629) (3.0.12)\n",
      "Requirement already satisfied, skipping upgrade: huggingface-hub<1.0,>=0.11.0 in /home/ubuntu/.local/lib/python3.8/site-packages (from transformers->transformers-neuronx==0.4.20230629) (0.15.1)\n",
      "Requirement already satisfied, skipping upgrade: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers->transformers-neuronx==0.4.20230629) (2023.3.23)\n",
      "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.8/dist-packages (from transformers->transformers-neuronx==0.4.20230629) (2.28.2)\n",
      "Requirement already satisfied, skipping upgrade: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers->transformers-neuronx==0.4.20230629) (0.13.3)\n",
      "Requirement already satisfied, skipping upgrade: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers->transformers-neuronx==0.4.20230629) (4.65.0)\n",
      "Requirement already satisfied, skipping upgrade: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch>=1.6.0->accelerate->transformers-neuronx==0.4.20230629) (4.5.0)\n",
      "Requirement already satisfied, skipping upgrade: nvidia-cuda-runtime-cu11==11.7.99; platform_system == \"Linux\" in /usr/local/lib/python3.8/dist-packages (from torch>=1.6.0->accelerate->transformers-neuronx==0.4.20230629) (11.7.99)\n",
      "Requirement already satisfied, skipping upgrade: nvidia-cudnn-cu11==8.5.0.96; platform_system == \"Linux\" in /usr/local/lib/python3.8/dist-packages (from torch>=1.6.0->accelerate->transformers-neuronx==0.4.20230629) (8.5.0.96)\n",
      "Requirement already satisfied, skipping upgrade: nvidia-cublas-cu11==11.10.3.66; platform_system == \"Linux\" in /usr/local/lib/python3.8/dist-packages (from torch>=1.6.0->accelerate->transformers-neuronx==0.4.20230629) (11.10.3.66)\n",
      "Requirement already satisfied, skipping upgrade: nvidia-cuda-nvrtc-cu11==11.7.99; platform_system == \"Linux\" in /usr/local/lib/python3.8/dist-packages (from torch>=1.6.0->accelerate->transformers-neuronx==0.4.20230629) (11.7.99)\n",
      "Requirement already satisfied, skipping upgrade: absl-py>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from torch-xla==1.13.0+torchneuron5->torch-neuronx->transformers-neuronx==0.4.20230629) (1.4.0)\n",
      "Requirement already satisfied, skipping upgrade: cloud-tpu-client>=0.10.0 in /usr/local/lib/python3.8/dist-packages (from torch-xla==1.13.0+torchneuron5->torch-neuronx->transformers-neuronx==0.4.20230629) (0.10)\n",
      "Requirement already satisfied, skipping upgrade: aws-neuronx-runtime-discovery~=2.0 in /usr/local/lib/python3.8/dist-packages (from libneuronxla==0.5.205->torch-neuronx->transformers-neuronx==0.4.20230629) (2.9)\n",
      "Requirement already satisfied, skipping upgrade: neuronx-cc~=2.0 in /usr/local/lib/python3.8/dist-packages (from libneuronxla==0.5.205->torch-neuronx->transformers-neuronx==0.4.20230629) (2.5.0.28+1be23f232)\n",
      "Requirement already satisfied, skipping upgrade: fsspec in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers->transformers-neuronx==0.4.20230629) (2023.4.0)\n",
      "Requirement already satisfied, skipping upgrade: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers->transformers-neuronx==0.4.20230629) (3.1.0)\n",
      "Requirement already satisfied, skipping upgrade: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->transformers->transformers-neuronx==0.4.20230629) (2.8)\n",
      "Requirement already satisfied, skipping upgrade: urllib3<1.27,>=1.21.1 in /usr/lib/python3/dist-packages (from requests->transformers->transformers-neuronx==0.4.20230629) (1.25.8)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->transformers->transformers-neuronx==0.4.20230629) (2019.11.28)\n",
      "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.8/dist-packages (from nvidia-cuda-runtime-cu11==11.7.99; platform_system == \"Linux\"->torch>=1.6.0->accelerate->transformers-neuronx==0.4.20230629) (67.6.1)\n",
      "Requirement already satisfied, skipping upgrade: wheel in /usr/lib/python3/dist-packages (from nvidia-cuda-runtime-cu11==11.7.99; platform_system == \"Linux\"->torch>=1.6.0->accelerate->transformers-neuronx==0.4.20230629) (0.34.2)\n",
      "Requirement already satisfied, skipping upgrade: google-api-python-client==1.8.0 in /usr/local/lib/python3.8/dist-packages (from cloud-tpu-client>=0.10.0->torch-xla==1.13.0+torchneuron5->torch-neuronx->transformers-neuronx==0.4.20230629) (1.8.0)\n",
      "Requirement already satisfied, skipping upgrade: oauth2client in /usr/local/lib/python3.8/dist-packages (from cloud-tpu-client>=0.10.0->torch-xla==1.13.0+torchneuron5->torch-neuronx->transformers-neuronx==0.4.20230629) (4.1.3)\n",
      "Requirement already satisfied, skipping upgrade: neuronx-hwm==2.5.0.0 in /usr/local/lib/python3.8/dist-packages (from neuronx-cc~=2.0->libneuronxla==0.5.205->torch-neuronx->transformers-neuronx==0.4.20230629) (2.5.0.0+dad732dd6)\n",
      "Requirement already satisfied, skipping upgrade: networkx<=2.6.3 in /usr/local/lib/python3.8/dist-packages (from neuronx-cc~=2.0->libneuronxla==0.5.205->torch-neuronx->transformers-neuronx==0.4.20230629) (2.6.3)\n",
      "Requirement already satisfied, skipping upgrade: scipy<=1.7.3 in /usr/local/lib/python3.8/dist-packages (from neuronx-cc~=2.0->libneuronxla==0.5.205->torch-neuronx->transformers-neuronx==0.4.20230629) (1.7.3)\n",
      "Requirement already satisfied, skipping upgrade: python-daemon>=2.2.4 in /usr/local/lib/python3.8/dist-packages (from neuronx-cc~=2.0->libneuronxla==0.5.205->torch-neuronx->transformers-neuronx==0.4.20230629) (3.0.1)\n",
      "Requirement already satisfied, skipping upgrade: requests-unixsocket>=0.1.5 in /usr/lib/python3/dist-packages (from neuronx-cc~=2.0->libneuronxla==0.5.205->torch-neuronx->transformers-neuronx==0.4.20230629) (0.2.0)\n",
      "Requirement already satisfied, skipping upgrade: islpy<=2022.1.1,>2021.1 in /usr/local/lib/python3.8/dist-packages (from neuronx-cc~=2.0->libneuronxla==0.5.205->torch-neuronx->transformers-neuronx==0.4.20230629) (2022.1.1)\n",
      "Requirement already satisfied, skipping upgrade: pgzip>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from neuronx-cc~=2.0->libneuronxla==0.5.205->torch-neuronx->transformers-neuronx==0.4.20230629) (0.3.4)\n",
      "Requirement already satisfied, skipping upgrade: httplib2<1dev,>=0.9.2 in /usr/lib/python3/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla==1.13.0+torchneuron5->torch-neuronx->transformers-neuronx==0.4.20230629) (0.14.0)\n",
      "Requirement already satisfied, skipping upgrade: google-auth>=1.4.1 in /usr/local/lib/python3.8/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla==1.13.0+torchneuron5->torch-neuronx->transformers-neuronx==0.4.20230629) (2.17.3)\n",
      "Requirement already satisfied, skipping upgrade: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.8/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla==1.13.0+torchneuron5->torch-neuronx->transformers-neuronx==0.4.20230629) (0.1.0)\n",
      "Requirement already satisfied, skipping upgrade: google-api-core<2dev,>=1.13.0 in /usr/local/lib/python3.8/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla==1.13.0+torchneuron5->torch-neuronx->transformers-neuronx==0.4.20230629) (1.34.0)\n",
      "Requirement already satisfied, skipping upgrade: six<2dev,>=1.6.1 in /usr/lib/python3/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla==1.13.0+torchneuron5->torch-neuronx->transformers-neuronx==0.4.20230629) (1.14.0)\n",
      "Requirement already satisfied, skipping upgrade: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla==1.13.0+torchneuron5->torch-neuronx->transformers-neuronx==0.4.20230629) (3.0.1)\n",
      "Requirement already satisfied, skipping upgrade: pyasn1>=0.1.7 in /usr/lib/python3/dist-packages (from oauth2client->cloud-tpu-client>=0.10.0->torch-xla==1.13.0+torchneuron5->torch-neuronx->transformers-neuronx==0.4.20230629) (0.4.2)\n",
      "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.0.5 in /usr/lib/python3/dist-packages (from oauth2client->cloud-tpu-client>=0.10.0->torch-xla==1.13.0+torchneuron5->torch-neuronx->transformers-neuronx==0.4.20230629) (0.2.1)\n",
      "Requirement already satisfied, skipping upgrade: rsa>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from oauth2client->cloud-tpu-client>=0.10.0->torch-xla==1.13.0+torchneuron5->torch-neuronx->transformers-neuronx==0.4.20230629) (4.9)\n",
      "Requirement already satisfied, skipping upgrade: docutils in /usr/local/lib/python3.8/dist-packages (from python-daemon>=2.2.4->neuronx-cc~=2.0->libneuronxla==0.5.205->torch-neuronx->transformers-neuronx==0.4.20230629) (0.19)\n",
      "Requirement already satisfied, skipping upgrade: lockfile>=0.10 in /usr/local/lib/python3.8/dist-packages (from python-daemon>=2.2.4->neuronx-cc~=2.0->libneuronxla==0.5.205->torch-neuronx->transformers-neuronx==0.4.20230629) (0.12.2)\n",
      "Requirement already satisfied, skipping upgrade: pytest>=2 in /usr/local/lib/python3.8/dist-packages (from islpy<=2022.1.1,>2021.1->neuronx-cc~=2.0->libneuronxla==0.5.205->torch-neuronx->transformers-neuronx==0.4.20230629) (7.3.1)\n",
      "Requirement already satisfied, skipping upgrade: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth>=1.4.1->google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla==1.13.0+torchneuron5->torch-neuronx->transformers-neuronx==0.4.20230629) (5.3.0)\n",
      "Requirement already satisfied, skipping upgrade: googleapis-common-protos<2.0dev,>=1.56.2 in /usr/local/lib/python3.8/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla==1.13.0+torchneuron5->torch-neuronx->transformers-neuronx==0.4.20230629) (1.59.0)\n",
      "Requirement already satisfied, skipping upgrade: iniconfig in /usr/local/lib/python3.8/dist-packages (from pytest>=2->islpy<=2022.1.1,>2021.1->neuronx-cc~=2.0->libneuronxla==0.5.205->torch-neuronx->transformers-neuronx==0.4.20230629) (2.0.0)\n",
      "Requirement already satisfied, skipping upgrade: pluggy<2.0,>=0.12 in /usr/local/lib/python3.8/dist-packages (from pytest>=2->islpy<=2022.1.1,>2021.1->neuronx-cc~=2.0->libneuronxla==0.5.205->torch-neuronx->transformers-neuronx==0.4.20230629) (1.0.0)\n",
      "Requirement already satisfied, skipping upgrade: exceptiongroup>=1.0.0rc8; python_version < \"3.11\" in /usr/local/lib/python3.8/dist-packages (from pytest>=2->islpy<=2022.1.1,>2021.1->neuronx-cc~=2.0->libneuronxla==0.5.205->torch-neuronx->transformers-neuronx==0.4.20230629) (1.1.1)\n",
      "Requirement already satisfied, skipping upgrade: tomli>=1.0.0; python_version < \"3.11\" in /usr/local/lib/python3.8/dist-packages (from pytest>=2->islpy<=2022.1.1,>2021.1->neuronx-cc~=2.0->libneuronxla==0.5.205->torch-neuronx->transformers-neuronx==0.4.20230629) (2.0.1)\n",
      "Building wheels for collected packages: transformers-neuronx\n",
      "  Building wheel for transformers-neuronx (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for transformers-neuronx: filename=transformers_neuronx-0.4.20230629-py3-none-any.whl size=122672 sha256=66e4cc9f0757da9ccfa7786eb8944dff2d11d4ca04c90a870e4dc17750bcc09b\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-abivopgt/wheels/a8/cd/08/7e54ef998d43ebf4954c9c66f5667a9801fec18049af641371\n",
      "Successfully built transformers-neuronx\n",
      "Installing collected packages: transformers-neuronx\n",
      "  Attempting uninstall: transformers-neuronx\n",
      "    Found existing installation: transformers-neuronx 0.4.20230629\n",
      "    Uninstalling transformers-neuronx-0.4.20230629:\n",
      "      Successfully uninstalled transformers-neuronx-0.4.20230629\n",
      "Successfully installed transformers-neuronx-0.4.20230629\n",
      "Collecting git+https://github.com/huggingface/optimum-neuron.git@neuron_model_for_causal_lm\n",
      "  Cloning https://github.com/huggingface/optimum-neuron.git (to revision neuron_model_for_causal_lm) to /tmp/pip-req-build-uhlftqer\n",
      "  Running command git clone -q https://github.com/huggingface/optimum-neuron.git /tmp/pip-req-build-uhlftqer\n",
      "  Running command git checkout -b neuron_model_for_causal_lm --track origin/neuron_model_for_causal_lm\n",
      "  Switched to a new branch 'neuron_model_for_causal_lm'\n",
      "  Branch 'neuron_model_for_causal_lm' set up to track remote branch 'neuron_model_for_causal_lm' from 'origin'.\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: transformers>=4.28.0 in /usr/local/lib/python3.8/dist-packages (from optimum-neuron==0.0.7.dev0) (4.28.1)\n",
      "Requirement already satisfied, skipping upgrade: accelerate>=0.20.1 in /home/ubuntu/.local/lib/python3.8/site-packages (from optimum-neuron==0.0.7.dev0) (0.20.3)\n",
      "Requirement already satisfied, skipping upgrade: optimum in /home/ubuntu/.local/lib/python3.8/site-packages (from optimum-neuron==0.0.7.dev0) (1.8.8)\n",
      "Requirement already satisfied, skipping upgrade: huggingface-hub>=0.14.0 in /home/ubuntu/.local/lib/python3.8/site-packages (from optimum-neuron==0.0.7.dev0) (0.15.1)\n",
      "Requirement already satisfied, skipping upgrade: numpy<=1.21.6,>=1.18.5 in /usr/local/lib/python3.8/dist-packages (from optimum-neuron==0.0.7.dev0) (1.21.6)\n",
      "Requirement already satisfied, skipping upgrade: protobuf<4 in /usr/local/lib/python3.8/dist-packages (from optimum-neuron==0.0.7.dev0) (3.20.2)\n",
      "Requirement already satisfied, skipping upgrade: filelock in /usr/lib/python3/dist-packages (from transformers>=4.28.0->optimum-neuron==0.0.7.dev0) (3.0.12)\n",
      "Requirement already satisfied, skipping upgrade: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers>=4.28.0->optimum-neuron==0.0.7.dev0) (23.1)\n",
      "Requirement already satisfied, skipping upgrade: pyyaml>=5.1 in /home/ubuntu/.local/lib/python3.8/site-packages (from transformers>=4.28.0->optimum-neuron==0.0.7.dev0) (6.0)\n",
      "Requirement already satisfied, skipping upgrade: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers>=4.28.0->optimum-neuron==0.0.7.dev0) (2023.3.23)\n",
      "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.8/dist-packages (from transformers>=4.28.0->optimum-neuron==0.0.7.dev0) (2.28.2)\n",
      "Requirement already satisfied, skipping upgrade: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers>=4.28.0->optimum-neuron==0.0.7.dev0) (0.13.3)\n",
      "Requirement already satisfied, skipping upgrade: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers>=4.28.0->optimum-neuron==0.0.7.dev0) (4.65.0)\n",
      "Requirement already satisfied, skipping upgrade: psutil in /usr/local/lib/python3.8/dist-packages (from accelerate>=0.20.1->optimum-neuron==0.0.7.dev0) (5.9.5)\n",
      "Requirement already satisfied, skipping upgrade: torch>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from accelerate>=0.20.1->optimum-neuron==0.0.7.dev0) (1.13.1)\n",
      "Requirement already satisfied, skipping upgrade: coloredlogs in /usr/local/lib/python3.8/dist-packages (from optimum->optimum-neuron==0.0.7.dev0) (15.0.1)\n",
      "Requirement already satisfied, skipping upgrade: sympy in /usr/local/lib/python3.8/dist-packages (from optimum->optimum-neuron==0.0.7.dev0) (1.11.1)\n",
      "Requirement already satisfied, skipping upgrade: torchvision in /usr/local/lib/python3.8/dist-packages (from optimum->optimum-neuron==0.0.7.dev0) (0.14.1)\n",
      "Requirement already satisfied, skipping upgrade: datasets in /usr/local/lib/python3.8/dist-packages (from optimum->optimum-neuron==0.0.7.dev0) (2.11.0)\n",
      "Requirement already satisfied, skipping upgrade: fsspec in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.14.0->optimum-neuron==0.0.7.dev0) (2023.4.0)\n",
      "Requirement already satisfied, skipping upgrade: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.14.0->optimum-neuron==0.0.7.dev0) (4.5.0)\n",
      "Requirement already satisfied, skipping upgrade: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers>=4.28.0->optimum-neuron==0.0.7.dev0) (3.1.0)\n",
      "Requirement already satisfied, skipping upgrade: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->transformers>=4.28.0->optimum-neuron==0.0.7.dev0) (2.8)\n",
      "Requirement already satisfied, skipping upgrade: urllib3<1.27,>=1.21.1 in /usr/lib/python3/dist-packages (from requests->transformers>=4.28.0->optimum-neuron==0.0.7.dev0) (1.25.8)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->transformers>=4.28.0->optimum-neuron==0.0.7.dev0) (2019.11.28)\n",
      "Requirement already satisfied, skipping upgrade: nvidia-cuda-runtime-cu11==11.7.99; platform_system == \"Linux\" in /usr/local/lib/python3.8/dist-packages (from torch>=1.6.0->accelerate>=0.20.1->optimum-neuron==0.0.7.dev0) (11.7.99)\n",
      "Requirement already satisfied, skipping upgrade: nvidia-cudnn-cu11==8.5.0.96; platform_system == \"Linux\" in /usr/local/lib/python3.8/dist-packages (from torch>=1.6.0->accelerate>=0.20.1->optimum-neuron==0.0.7.dev0) (8.5.0.96)\n",
      "Requirement already satisfied, skipping upgrade: nvidia-cublas-cu11==11.10.3.66; platform_system == \"Linux\" in /usr/local/lib/python3.8/dist-packages (from torch>=1.6.0->accelerate>=0.20.1->optimum-neuron==0.0.7.dev0) (11.10.3.66)\n",
      "Requirement already satisfied, skipping upgrade: nvidia-cuda-nvrtc-cu11==11.7.99; platform_system == \"Linux\" in /usr/local/lib/python3.8/dist-packages (from torch>=1.6.0->accelerate>=0.20.1->optimum-neuron==0.0.7.dev0) (11.7.99)\n",
      "Requirement already satisfied, skipping upgrade: humanfriendly>=9.1 in /usr/local/lib/python3.8/dist-packages (from coloredlogs->optimum->optimum-neuron==0.0.7.dev0) (10.0)\n",
      "Requirement already satisfied, skipping upgrade: mpmath>=0.19 in /usr/local/lib/python3.8/dist-packages (from sympy->optimum->optimum-neuron==0.0.7.dev0) (1.3.0)\n",
      "Requirement already satisfied, skipping upgrade: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision->optimum->optimum-neuron==0.0.7.dev0) (9.5.0)\n",
      "Requirement already satisfied, skipping upgrade: pyarrow>=8.0.0 in /usr/local/lib/python3.8/dist-packages (from datasets->optimum->optimum-neuron==0.0.7.dev0) (11.0.0)\n",
      "Requirement already satisfied, skipping upgrade: dill<0.3.7,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from datasets->optimum->optimum-neuron==0.0.7.dev0) (0.3.6)\n",
      "Requirement already satisfied, skipping upgrade: pandas in /usr/local/lib/python3.8/dist-packages (from datasets->optimum->optimum-neuron==0.0.7.dev0) (2.0.0)\n",
      "Requirement already satisfied, skipping upgrade: xxhash in /usr/local/lib/python3.8/dist-packages (from datasets->optimum->optimum-neuron==0.0.7.dev0) (3.2.0)\n",
      "Requirement already satisfied, skipping upgrade: multiprocess in /usr/local/lib/python3.8/dist-packages (from datasets->optimum->optimum-neuron==0.0.7.dev0) (0.70.14)\n",
      "Requirement already satisfied, skipping upgrade: aiohttp in /usr/local/lib/python3.8/dist-packages (from datasets->optimum->optimum-neuron==0.0.7.dev0) (3.8.4)\n",
      "Requirement already satisfied, skipping upgrade: responses<0.19 in /usr/local/lib/python3.8/dist-packages (from datasets->optimum->optimum-neuron==0.0.7.dev0) (0.18.0)\n",
      "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.8/dist-packages (from nvidia-cuda-runtime-cu11==11.7.99; platform_system == \"Linux\"->torch>=1.6.0->accelerate>=0.20.1->optimum-neuron==0.0.7.dev0) (67.6.1)\n",
      "Requirement already satisfied, skipping upgrade: wheel in /usr/lib/python3/dist-packages (from nvidia-cuda-runtime-cu11==11.7.99; platform_system == \"Linux\"->torch>=1.6.0->accelerate>=0.20.1->optimum-neuron==0.0.7.dev0) (0.34.2)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil>=2.8.2 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets->optimum->optimum-neuron==0.0.7.dev0) (2.8.2)\n",
      "Requirement already satisfied, skipping upgrade: pytz>=2020.1 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets->optimum->optimum-neuron==0.0.7.dev0) (2023.3)\n",
      "Requirement already satisfied, skipping upgrade: tzdata>=2022.1 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets->optimum->optimum-neuron==0.0.7.dev0) (2023.3)\n",
      "Requirement already satisfied, skipping upgrade: attrs>=17.3.0 in /home/ubuntu/.local/lib/python3.8/site-packages (from aiohttp->datasets->optimum->optimum-neuron==0.0.7.dev0) (23.1.0)\n",
      "Requirement already satisfied, skipping upgrade: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->optimum->optimum-neuron==0.0.7.dev0) (6.0.4)\n",
      "Requirement already satisfied, skipping upgrade: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->optimum->optimum-neuron==0.0.7.dev0) (4.0.2)\n",
      "Requirement already satisfied, skipping upgrade: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->optimum->optimum-neuron==0.0.7.dev0) (1.8.2)\n",
      "Requirement already satisfied, skipping upgrade: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->optimum->optimum-neuron==0.0.7.dev0) (1.3.3)\n",
      "Requirement already satisfied, skipping upgrade: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->optimum->optimum-neuron==0.0.7.dev0) (1.3.1)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->datasets->optimum->optimum-neuron==0.0.7.dev0) (1.14.0)\n",
      "Building wheels for collected packages: optimum-neuron\n",
      "  Building wheel for optimum-neuron (PEP 517) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for optimum-neuron: filename=optimum_neuron-0.0.7.dev0-py3-none-any.whl size=122076 sha256=2552e96c33b05df7db1672dc913268ec3f42040627b9b62f426017baeab7f1a0\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-aya31x9y/wheels/69/41/b4/a333fd0f93b85fe2bdfefd8a851b99e4ffcda0f015e6c4fdff\n",
      "Successfully built optimum-neuron\n",
      "Installing collected packages: optimum-neuron\n",
      "  Attempting uninstall: optimum-neuron\n",
      "    Found existing installation: optimum-neuron 0.0.6.dev0\n",
      "    Uninstalling optimum-neuron-0.0.6.dev0:\n",
      "      Successfully uninstalled optimum-neuron-0.0.6.dev0\n",
      "Successfully installed optimum-neuron-0.0.7.dev0\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/aws-neuron/transformers-neuronx.git --upgrade\n",
    "!pip install git+https://github.com/huggingface/optimum-neuron.git@neuron_model_for_causal_lm --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "291aac28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..\n",
      "Compiler status PASS\n",
      "2023-Jun-29 12:05:43.0738 2074:2074 [0] nccl_net_ofi_init:1415 CCOM WARN NET/OFI aws-ofi-nccl initialization failed\n",
      "2023-Jun-29 12:05:43.0738 2074:2074 [0] init.cc:99 CCOM WARN OFI plugin initNet() failed is EFA enabled?\n",
      "HF model converted to Neuron\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello, I\\'m a language model, and I\\'d love to introduce you to my upcoming writing course,\" she said, speaking of her next writing book, \"Beyond Borders: A New Translation of Shakespeare\\'s Old Folio Series.\" \"I\\'m really looking forward to this learning time as well,\" she added. \"I have no plans to become a linguistic learner anytime soon.\"\\n\\nAlfred Wiesel, an English professor and linguist at the University of Virginia, agrees: \"There is nothing in American studies quite like watching a Shakespeare play as a teacher or as a citizen.\" However, he added: \"American authors frequently', \"Hello, I'm a language model, so I wanted to make a code base for working with C#. I think it's incredibly powerful and straightforward. So my thought was:\\n\\nA single implementation is possibleâ€¦\\n\\nâ€¦but let's be bold and say we are working with C# and using something like C# 5.10. I want a single instance.\\n\\nWe know that we need a single instance, so I will create one.\\n\\nIn this case, I will create an example of a program, to show you how to use the API of a programming language.\\n\\nIn that example you are\"]\n",
      "Outputs generated using AWS sampling loop\n",
      "[\"Hello, I'm a language model, not a syntax model. I am not a language model, but a syntax model that's very good at it. I'm not a language model but a syntax model that's great at it. It's not perfect, but it's perfect.\\n\\nQ: What are some of the ways you're coming in and out of the language model?\\n\\nA: I'm coming in from a few places, so I'm coming in from all kinds of places. So I'm coming in from a lot of places. So I'm coming in from a lot of places. And I'm coming\", 'Hello, I\\'m a language model, and I have a lot of ideas. I have some really cool ideas, but I don\\'t know enough about them to make a complete picture.\\n\\n\"I have some really cool ideas, but I don\\'t know enough about them to make a complete picture. A lot of people call me a \"model\", and they\\'re very polite, but I don\\'t really know a lot about them. I have a lot of ideas, but I don\\'t know enough about them to make a complete picture. I still don\\'t know something about the language model. I don\\'t know the language model in']\n",
      "Outputs generated using HF generate\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from optimum.neuron import NeuronModelForCausalLM\n",
    "\n",
    "\n",
    "os.environ[\"NEURON_CC_FLAGS\"] = \"--model-type=transformer-inference\"\n",
    "\n",
    "# Compilation does not work with batch_size = 1\n",
    "batch_size = 2\n",
    "seq_length = 128\n",
    "\n",
    "# Load and convert the Hub model to Neuron format\n",
    "model_neuron = NeuronModelForCausalLM.from_pretrained(\n",
    "    \"gpt2\", batch_size=batch_size, sequence_length=seq_length, export=True, tp_degree=2, amp=\"f32\"\n",
    ")\n",
    "\n",
    "print(\"HF model converted to Neuron\")\n",
    "\n",
    "# Get a tokenizer and example input\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "prompt_text = \"Hello, I'm a language model,\"\n",
    "# We need to replicate the text because batch_size is not 1\n",
    "prompts = [prompt_text for _ in range(batch_size)]\n",
    "\n",
    "# Encode text and generate using AWS sampling loop\n",
    "encoded_text = tokenizer(prompts, return_tensors='pt')\n",
    "with torch.inference_mode():\n",
    "    generated_sequence = model_neuron.model.sample(encoded_text.input_ids, sequence_length=seq_length)\n",
    "    print([tokenizer.decode(tok) for tok in generated_sequence])\n",
    "\n",
    "print(\"Outputs generated using AWS sampling loop\")\n",
    "\n",
    "# Specifiy padding options\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "# Encode tokens and generate using temperature\n",
    "tokens = tokenizer(prompts, padding=True, return_tensors='pt')\n",
    "model_neuron.reset_generation() # Need to check if this can be automated\n",
    "sample_output = model_neuron.generate(\n",
    "    **tokens,\n",
    "    do_sample=True,\n",
    "    max_length=seq_length,\n",
    "    temperature=0.7,\n",
    ")\n",
    "print([tokenizer.decode(tok) for tok in sample_output])\n",
    "\n",
    "print(\"Outputs generated using HF generate\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e3db68e5",
   "metadata": {},
   "source": [
    "## 1. Convert BERT to AWS Neuron (Inferentia2) with `optimum-neuron`\n",
    "\n",
    "We are going to use the [optimum-neuron](https://huggingface.co/docs/optimum-neuron/index). ðŸ¤— Optimum Neuron is the interface between the ðŸ¤— Transformers library and AWS Accelerators including [AWS Trainium](https://aws.amazon.com/machine-learning/trainium/?nc1=h_ls) and [AWS Inferentia](https://aws.amazon.com/machine-learning/inferentia/?nc1=h_ls). It provides a set of tools enabling easy model loading, training and inference on single- and multi-Accelerator settings for different downstream tasks. \n",
    "\n",
    "As a first step, we need to install the `optimum-neuron` and other required packages.\n",
    "\n",
    "*Tip: If you are using Amazon SageMaker Notebook Instances or Studio you can go with the `conda_python3` conda kernel.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "39e3729c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/aws/sagemaker-python-sdk.git\n",
      "  Cloning https://github.com/aws/sagemaker-python-sdk.git to /tmp/pip-req-build-t4tkqum2\n",
      "  Running command git clone -q https://github.com/aws/sagemaker-python-sdk.git /tmp/pip-req-build-t4tkqum2\n",
      "Requirement already satisfied, skipping upgrade: attrs<24,>=23.1.0 in /home/ubuntu/.local/lib/python3.8/site-packages (from sagemaker==2.168.1.dev0) (23.1.0)\n",
      "Requirement already satisfied, skipping upgrade: boto3<2.0,>=1.26.131 in /home/ubuntu/.local/lib/python3.8/site-packages (from sagemaker==2.168.1.dev0) (1.26.161)\n",
      "Requirement already satisfied, skipping upgrade: cloudpickle==2.2.1 in /home/ubuntu/.local/lib/python3.8/site-packages (from sagemaker==2.168.1.dev0) (2.2.1)\n",
      "Requirement already satisfied, skipping upgrade: google-pasta in /home/ubuntu/.local/lib/python3.8/site-packages (from sagemaker==2.168.1.dev0) (0.2.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy<2.0,>=1.9.0 in /usr/local/lib/python3.8/dist-packages (from sagemaker==2.168.1.dev0) (1.21.6)\n",
      "Requirement already satisfied, skipping upgrade: protobuf<4.0,>=3.1 in /usr/local/lib/python3.8/dist-packages (from sagemaker==2.168.1.dev0) (3.20.2)\n",
      "Requirement already satisfied, skipping upgrade: protobuf3-to-dict<1.0,>=0.1.5 in /home/ubuntu/.local/lib/python3.8/site-packages (from sagemaker==2.168.1.dev0) (0.1.5)\n",
      "Requirement already satisfied, skipping upgrade: smdebug_rulesconfig==1.0.1 in /home/ubuntu/.local/lib/python3.8/site-packages (from sagemaker==2.168.1.dev0) (1.0.1)\n",
      "Requirement already satisfied, skipping upgrade: importlib-metadata<5.0,>=1.4.0 in /home/ubuntu/.local/lib/python3.8/site-packages (from sagemaker==2.168.1.dev0) (4.13.0)\n",
      "Requirement already satisfied, skipping upgrade: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from sagemaker==2.168.1.dev0) (23.1)\n",
      "Requirement already satisfied, skipping upgrade: pandas in /usr/local/lib/python3.8/dist-packages (from sagemaker==2.168.1.dev0) (2.0.0)\n",
      "Requirement already satisfied, skipping upgrade: pathos in /home/ubuntu/.local/lib/python3.8/site-packages (from sagemaker==2.168.1.dev0) (0.3.0)\n",
      "Requirement already satisfied, skipping upgrade: schema in /home/ubuntu/.local/lib/python3.8/site-packages (from sagemaker==2.168.1.dev0) (0.7.5)\n",
      "Requirement already satisfied, skipping upgrade: PyYAML==6.0 in /home/ubuntu/.local/lib/python3.8/site-packages (from sagemaker==2.168.1.dev0) (6.0)\n",
      "Requirement already satisfied, skipping upgrade: jsonschema in /usr/lib/python3/dist-packages (from sagemaker==2.168.1.dev0) (3.2.0)\n",
      "Requirement already satisfied, skipping upgrade: platformdirs in /usr/local/lib/python3.8/dist-packages (from sagemaker==2.168.1.dev0) (3.2.0)\n",
      "Requirement already satisfied, skipping upgrade: tblib==1.7.0 in /home/ubuntu/.local/lib/python3.8/site-packages (from sagemaker==2.168.1.dev0) (1.7.0)\n",
      "Requirement already satisfied, skipping upgrade: s3transfer<0.7.0,>=0.6.0 in /home/ubuntu/.local/lib/python3.8/site-packages (from boto3<2.0,>=1.26.131->sagemaker==2.168.1.dev0) (0.6.1)\n",
      "Requirement already satisfied, skipping upgrade: jmespath<2.0.0,>=0.7.1 in /home/ubuntu/.local/lib/python3.8/site-packages (from boto3<2.0,>=1.26.131->sagemaker==2.168.1.dev0) (1.0.1)\n",
      "Requirement already satisfied, skipping upgrade: botocore<1.30.0,>=1.29.161 in /home/ubuntu/.local/lib/python3.8/site-packages (from boto3<2.0,>=1.26.131->sagemaker==2.168.1.dev0) (1.29.161)\n",
      "Requirement already satisfied, skipping upgrade: six in /usr/lib/python3/dist-packages (from google-pasta->sagemaker==2.168.1.dev0) (1.14.0)\n",
      "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/lib/python3/dist-packages (from importlib-metadata<5.0,>=1.4.0->sagemaker==2.168.1.dev0) (1.0.0)\n",
      "Requirement already satisfied, skipping upgrade: tzdata>=2022.1 in /usr/local/lib/python3.8/dist-packages (from pandas->sagemaker==2.168.1.dev0) (2023.3)\n",
      "Requirement already satisfied, skipping upgrade: pytz>=2020.1 in /usr/local/lib/python3.8/dist-packages (from pandas->sagemaker==2.168.1.dev0) (2023.3)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil>=2.8.2 in /usr/local/lib/python3.8/dist-packages (from pandas->sagemaker==2.168.1.dev0) (2.8.2)\n",
      "Requirement already satisfied, skipping upgrade: dill>=0.3.6 in /usr/local/lib/python3.8/dist-packages (from pathos->sagemaker==2.168.1.dev0) (0.3.6)\n",
      "Requirement already satisfied, skipping upgrade: pox>=0.3.2 in /home/ubuntu/.local/lib/python3.8/site-packages (from pathos->sagemaker==2.168.1.dev0) (0.3.2)\n",
      "Requirement already satisfied, skipping upgrade: multiprocess>=0.70.14 in /usr/local/lib/python3.8/dist-packages (from pathos->sagemaker==2.168.1.dev0) (0.70.14)\n",
      "Requirement already satisfied, skipping upgrade: ppft>=1.7.6.6 in /home/ubuntu/.local/lib/python3.8/site-packages (from pathos->sagemaker==2.168.1.dev0) (1.7.6.6)\n",
      "Requirement already satisfied, skipping upgrade: contextlib2>=0.5.5 in /home/ubuntu/.local/lib/python3.8/site-packages (from schema->sagemaker==2.168.1.dev0) (21.6.0)\n",
      "Requirement already satisfied, skipping upgrade: urllib3<1.27,>=1.25.4 in /usr/lib/python3/dist-packages (from botocore<1.30.0,>=1.29.161->boto3<2.0,>=1.26.131->sagemaker==2.168.1.dev0) (1.25.8)\n",
      "Building wheels for collected packages: sagemaker\n",
      "  Building wheel for sagemaker (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sagemaker: filename=sagemaker-2.168.1.dev0-py2.py3-none-any.whl size=1152959 sha256=36980350bb08cf9ba0efca10ac90bec3877c3c892e81fe377044a848c334eda9\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-tyn1ws_0/wheels/86/90/ca/c446e4ac09f7ad1b813fe4ab437ffc09821067a2de18621ac6\n",
      "Successfully built sagemaker\n",
      "Installing collected packages: sagemaker\n",
      "  Attempting uninstall: sagemaker\n",
      "    Found existing installation: sagemaker 2.168.0\n",
      "    Uninstalling sagemaker-2.168.0:\n",
      "      Successfully uninstalled sagemaker-2.168.0\n",
      "Successfully installed sagemaker-2.168.1.dev0\n"
     ]
    }
   ],
   "source": [
    "!python -m pip install \"git+https://github.com/aws/sagemaker-python-sdk.git\"  --upgrade\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c59d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the required packages\n",
    "# !pip install \"optimum-neuron[neuronx]==0.0.6\"  --upgrade\n",
    "!pip install \"git+https://github.com/huggingface/optimum-neuron.git@b94d534cc0160f1e199fae6ae3a1c7b804b49e30\"  --upgrade\n",
    "\n",
    "# !python -m pip install \"sagemaker==2.169.0\"  --upgrade\n",
    "!python -m pip install \"git+https://github.com/aws/sagemaker-python-sdk.git\"  --upgrade\n",
    "# pip install sagemaker from github"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ce0ef431",
   "metadata": {},
   "source": [
    "After we have installed the `optimum-neuron` we can convert load and convert our model.\n",
    "\n",
    "We are going to use the [yiyanghkust/finbert-tone](https://huggingface.co/yiyanghkust/finbert-tone) model. FinBERT is a BERT model pre-trained on financial communication text. The purpose is to enhance financial NLP research and practice. It is trained on the following three financial communication corpus. The total corpora size is 4.9B tokens. This released finbert-tone model is the FinBERT model fine-tuned on 10,000 manually annotated (positive, negative, neutral) sentences from analyst reports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d8dfea",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"yiyanghkust/finbert-tone\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9e4386d9",
   "metadata": {},
   "source": [
    "At the time of writing, the [AWS Inferentia2 does not support dynamic shapes for inference](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/arch/neuron-features/dynamic-shapes.html?highlight=dynamic%20shapes#), which means that the input size needs to be static for compiling and inference. \n",
    "\n",
    "In simpler terms, this means when the model is converted with a sequence length of 16. The model can only run inference on inputs with the same shape. We are going to use the `optimum-cli` to convert our model with a sequence length of 128 and a batch size of 1. \n",
    "\n",
    "_When using a `t2.medium` instance the compiling takes around 2-3 minutes_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44ff097",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$model_id\"\n",
    "MODEL_ID=$1\n",
    "SEQUENCE_LENGTH=128\n",
    "BATCH_SIZE=1\n",
    "OUTPUT_DIR=tmp/ # used to store temproary files\n",
    "echo \"Model ID: $MODEL_ID\"\n",
    "\n",
    "# exporting model\n",
    "optimum-cli export neuron \\\n",
    "  --model $MODEL_ID \\\n",
    "  --sequence_length $SEQUENCE_LENGTH \\\n",
    "  --batch_size $BATCH_SIZE \\\n",
    "  $OUTPUT_DIR"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9997e9db",
   "metadata": {},
   "source": [
    "## 2. Create a custom `inference.py` script for `text-classification`\n",
    "\n",
    "The [Hugging Face Inference Toolkit](https://github.com/aws/sagemaker-huggingface-inference-toolkit) supports zero-code deployments on top of theÂ [pipelineÂ feature](https://huggingface.co/transformers/main_classes/pipelines.html)Â from ðŸ¤— Transformers. This allows users to deploy Hugging Face transformers without an inference script [[Example](https://github.com/huggingface/notebooks/blob/master/sagemaker/11_deploy_model_from_hf_hub/deploy_transformer_model_from_hf_hub.ipynb)]. \n",
    "\n",
    "Currently is this feature not supported with AWS Inferentia2, which means we need to provide an `inference.py` for running inference. But `optimum-neuron` has integrated support for the ðŸ¤— Transformers pipeline feature. That way we can use the `optimum-neuron` to create a pipeline for our model.\n",
    "\n",
    "If you want to know more about the `inference.py`Â script check out this [example](https://github.com/huggingface/notebooks/blob/master/sagemaker/17_custom_inference_script/sagemaker-notebook.ipynb). It explains amongst other things what the `model_fn` and `predict_fn` are. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4246c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir code"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "39f74625",
   "metadata": {},
   "source": [
    "In addition to our `inference.py` script we need to provide a `requirements.txt`, which installs the latest version of the `optimum-neuron` package, which comes with `pipeline` support for AWS Inferentia2. \n",
    "_Note: This is a temporary solution until the `optimum-neuron` package is updated inside the DLC._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e09fd8e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing code/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile code/requirements.txt\n",
    "git+https://github.com/aws-neuron/transformers-neuronx.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce675df9",
   "metadata": {},
   "source": [
    "We are using the `NEURON_RT_NUM_CORES=1` to make sure that each HTTP worker uses 1 Neuron core to maximize throughput."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ce41529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing code/inference.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile code/inference.py\n",
    "import os\n",
    "from transformers_neuronx.gptj.model import GPTJForSampling\n",
    "from transformers_neuronx.generation_utils import HuggingFaceGenerationModelAdapter\n",
    "from transformers_neuronx.module import save_pretrained_split\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "os.environ['NEURON_CC_FLAGS'] = '--model-type=transformer-inference'\n",
    "# Load and save the CPU model\n",
    "split_dir='gptj-split'\n",
    "model_id='EleutherAI/gpt-j-6b'\n",
    "revision='sharded'\n",
    "\n",
    "####### LOAD AND COMPILE THE MODEL #######\n",
    "model_cpu = AutoModelForCausalLM.from_pretrained(model_id, low_cpu_mem_usage=True, revision=revision)\n",
    "save_pretrained_split(model_cpu, split_dir)\n",
    "\n",
    "# Create and compile the Neuron model\n",
    "model = GPTJForSampling.from_pretrained(split_dir, batch_size=1, tp_degree=2, n_positions=512, amp='f32', unroll=None)\n",
    "model.to_neuron()\n",
    "model = HuggingFaceGenerationModelAdapter(model_cpu.config, model)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.padding_side = 'left'\n",
    "# https://huggingface.co/amazon/LightGPT\n",
    "\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    return model, tokenizer\n",
    "\n",
    "def predict_fn(data, model_tokenizer):\n",
    "    model, tokenizer = model_tokenizer\n",
    "    inputs = data.pop(\"inputs\", data)\n",
    "    parameters = data.pop(\"parameters\", None)\n",
    "\n",
    "    # preprocess\n",
    "    input_ids = tokenizer(inputs, return_tensors=\"pt\").input_ids\n",
    "\n",
    "    # pass inputs with all kwargs in data\n",
    "    model.reset_generation()\n",
    "    if parameters is not None:\n",
    "        outputs = model.generate(input_ids, **parameters)\n",
    "    else:\n",
    "        outputs = model.generate(input_ids, do_sample=True, temperature=0.7)\n",
    "\n",
    "    # postprocess the prediction\n",
    "    prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    return [{\"generated_text\": prediction}]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "144d8ccb",
   "metadata": {},
   "source": [
    "## 3. Upload the neuron model and inference script to Amazon S3\n",
    "\n",
    "Before we can deploy our neuron model to Amazon SageMaker we need to create a `model.tar.gz` archive with all our model artifacts saved into, e.g.Â `model.neuron` and upload this to Amazon S3.\n",
    "\n",
    "To do this we need to set up our permissions. Currently `inf2` instances are only available in the `us-east-2` region [[REF](https://aws.amazon.com/de/about-aws/whats-new/2023/05/sagemaker-ml-inf2-ml-trn1-instances-model-deployment/)]. Therefore we need to force the region to us-east-2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d016feac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "os.environ[\"AWS_DEFAULT_REGION\"] = \"us-east-2\" # need to set to ohio region"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "83ea3df1",
   "metadata": {},
   "source": [
    "Now lets create our SageMaker session and upload our model to Amazon S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0970b518",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"AWS_ACCESS_KEY_ID\"] = \"AKIAYD4NIUHMQLQQYXHF\"\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "952983b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Couldn't call 'get_role' to get Role ARN from role name philippschmid to get Role path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker role arn: arn:aws:iam::558105141721:role/sagemaker_execution_role\n",
      "sagemaker bucket: sagemaker-us-east-2-558105141721\n",
      "sagemaker session region: us-east-2\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")\n",
    "assert sess.boto_region_name == \"us-east-2\", \"region must be us-east-2\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374ff630",
   "metadata": {},
   "source": [
    "Next, we create our `model.tar.gz`.TheÂ `inference.py`Â script will be placed into aÂ `code/`Â folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dbb67937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'gptj-transformers'\n",
      "/home/ubuntu/huggingface-inferentia2-samples/gptj-transformers\n"
     ]
    }
   ],
   "source": [
    "%cd gptj-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a3808b9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/huggingface-inferentia2-samples/gptj-transformers/tmp\n",
      "code/\n",
      "code/inference.py\n",
      "code/requirements.txt\n",
      "/home/ubuntu/huggingface-inferentia2-samples/gptj-transformers\n"
     ]
    }
   ],
   "source": [
    "# copy inference.py into the code/ directory of the model directory.\n",
    "!mkdir -p tmp\n",
    "!cp -r code/ tmp/code/\n",
    "# create a model.tar.gz archive with all the model artifacts and the inference.py script.\n",
    "%cd tmp\n",
    "!tar zcvf model.tar.gz *\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a6f330",
   "metadata": {},
   "source": [
    "Now we can upload our `model.tar.gz` to our session S3 bucket with `sagemaker`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6146af09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model artifcats uploaded to s3://sagemaker-us-east-2-558105141721/neuronx/gptj/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.s3 import S3Uploader\n",
    "\n",
    "# create s3 uri\n",
    "s3_model_path = f\"s3://{sess.default_bucket()}/neuronx/gptj\"\n",
    "\n",
    "# upload model.tar.gz\n",
    "s3_model_uri = S3Uploader.upload(local_path=\"tmp/model.tar.gz\",desired_s3_uri=s3_model_path)\n",
    "print(f\"model artifcats uploaded to {s3_model_uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cccbb719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean tmp directory after uploading\n",
    "# !rm -rf tmp"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "04e1395a",
   "metadata": {},
   "source": [
    "## 4. Deploy a Real-time Inference Endpoint on Amazon SageMaker\n",
    "\n",
    "After we have uploaded ourÂ `model.tar.gz`Â to Amazon S3 can we create a customÂ `HuggingfaceModel`. This class will be used to create and deploy our real-time inference endpoint on Amazon SageMaker.\n",
    "\n",
    "The `inf2.xlarge` instance type is the smallest instance type with AWS Inferentia2 support. It comes with 1 Inferentia2 chip with 2 Neuron Cores. This means we can use 2 Model server workers to maximize throughput and run 2 inferences in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "41522ef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------!"
     ]
    }
   ],
   "source": [
    "from sagemaker.huggingface.model import HuggingFaceModel\n",
    "\n",
    "# create Hugging Face Model Class\n",
    "huggingface_model = HuggingFaceModel(\n",
    "   model_data=s3_model_uri,        # path to your model and script\n",
    "   role=role,                      # iam role with permissions to create an Endpoint\n",
    "   transformers_version=\"4.28.1\",  # transformers version used\n",
    "   pytorch_version=\"1.13.0\",       # pytorch version used\n",
    "   py_version='py38',              # python version used\n",
    "   model_server_workers=1,         # number of workers for the model server\n",
    ")\n",
    "\n",
    "# Let SageMaker know that we've already compiled the model\n",
    "huggingface_model._is_compiled_model = True\n",
    "\n",
    "# deploy the endpoint endpoint\n",
    "predictor = huggingface_model.deploy(\n",
    "    initial_instance_count=1,      # number of instances\n",
    "    instance_type=\"ml.inf2.8xlarge\" # AWS Inferentia Instance\n",
    "    timeout\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c858560",
   "metadata": {},
   "source": [
    "# 5. Run and evaluate Inference performance of BERT on Inferentia\n",
    "\n",
    "TheÂ `.deploy()`Â returns anÂ `HuggingFacePredictor`Â object which can be used to request inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "da2ff049",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModelError",
     "evalue": "An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (0) from primary with message \"Your invocation timed out while waiting for a response from container primary. Review the latency metrics for each container in Amazon CloudWatch, resolve the issue, and try again.\". See https://us-east-2.console.aws.amazon.com/cloudwatch/home?region=us-east-2#logEventViewer:group=/aws/sagemaker/Endpoints/huggingface-pytorch-inference-neuronx-m-2023-06-29-14-46-49-807 in account 558105141721 for more information.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModelError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m data \u001b[39m=\u001b[39m {\n\u001b[1;32m      2\u001b[0m   \u001b[39m\"\u001b[39m\u001b[39minputs\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mthe mesmerizing performances of the leads keep the film grounded and keep the audience riveted .\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m      3\u001b[0m }\n\u001b[0;32m----> 5\u001b[0m res \u001b[39m=\u001b[39m predictor\u001b[39m.\u001b[39;49mpredict(data\u001b[39m=\u001b[39;49mdata)\n\u001b[1;32m      6\u001b[0m res\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sagemaker/base_predictor.py:167\u001b[0m, in \u001b[0;36mPredictor.predict\u001b[0;34m(self, data, initial_args, target_model, target_variant, inference_id)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Return the inference from the specified endpoint.\u001b[39;00m\n\u001b[1;32m    138\u001b[0m \n\u001b[1;32m    139\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[39m        as is.\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    164\u001b[0m request_args \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_request_args(\n\u001b[1;32m    165\u001b[0m     data, initial_args, target_model, target_variant, inference_id\n\u001b[1;32m    166\u001b[0m )\n\u001b[0;32m--> 167\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msagemaker_session\u001b[39m.\u001b[39;49msagemaker_runtime_client\u001b[39m.\u001b[39;49minvoke_endpoint(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mrequest_args)\n\u001b[1;32m    168\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle_response(response)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/botocore/client.py:530\u001b[0m, in \u001b[0;36mClientCreator._create_api_method.<locals>._api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    526\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m    527\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mpy_operation_name\u001b[39m}\u001b[39;00m\u001b[39m() only accepts keyword arguments.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    528\u001b[0m     )\n\u001b[1;32m    529\u001b[0m \u001b[39m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n\u001b[0;32m--> 530\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_api_call(operation_name, kwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/botocore/client.py:964\u001b[0m, in \u001b[0;36mBaseClient._make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    962\u001b[0m     error_code \u001b[39m=\u001b[39m parsed_response\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mError\u001b[39m\u001b[39m\"\u001b[39m, {})\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mCode\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    963\u001b[0m     error_class \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexceptions\u001b[39m.\u001b[39mfrom_code(error_code)\n\u001b[0;32m--> 964\u001b[0m     \u001b[39mraise\u001b[39;00m error_class(parsed_response, operation_name)\n\u001b[1;32m    965\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    966\u001b[0m     \u001b[39mreturn\u001b[39;00m parsed_response\n",
      "\u001b[0;31mModelError\u001b[0m: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (0) from primary with message \"Your invocation timed out while waiting for a response from container primary. Review the latency metrics for each container in Amazon CloudWatch, resolve the issue, and try again.\". See https://us-east-2.console.aws.amazon.com/cloudwatch/home?region=us-east-2#logEventViewer:group=/aws/sagemaker/Endpoints/huggingface-pytorch-inference-neuronx-m-2023-06-29-14-46-49-807 in account 558105141721 for more information."
     ]
    }
   ],
   "source": [
    "data = {\n",
    "  \"inputs\": \"the mesmerizing performances of the leads keep the film grounded and keep the audience riveted .\",\n",
    "}\n",
    "\n",
    "res = predictor.predict(data=data)\n",
    "res"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0a146346",
   "metadata": {},
   "source": [
    "We managed to deploy our neuron compiled BERT to AWS Inferentia on Amazon SageMaker. Now, let's test its performance of it. As a dummy load test will we use threading to send 10000 requests to our endpoint with 10 threads.\n",
    "\n",
    "_Note: When running the load test we environment was based in europe and the endpoint is deployed in us-east-2._"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1030c87f",
   "metadata": {},
   "source": [
    "### Delete model and endpoint\n",
    "\n",
    "To clean up, we can delete the model and endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d8917d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_model()\n",
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b656e81d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c281c456f1b8161c8906f4af2c08ed2c40c50136979eaae69688b01f70e9f4a9"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
