{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "02e90d5c",
   "metadata": {},
   "source": [
    "# Deploy Stable Diffusion on AWS inferentia2 with Amazon SageMaker\n",
    "\n",
    "In this end-to-end tutorial, you will learn how to speed up BERT inference down to `1ms` latency for text classification with Hugging Face Transformers, Amazon SageMaker, and AWS Inferentia2.\n",
    "\n",
    "You will learn how to: \n",
    "\n",
    "1. Convert BERT to AWS Neuron (Inferentia2) with `optimum-neuron`\n",
    "2. Create a custom `inference.py` script for `text-classification`\n",
    "3. Upload the neuron model and inference script to Amazon S3\n",
    "4. Deploy a Real-time Inference Endpoint on Amazon SageMaker\n",
    "5. Run and evaluate Inference performance of BERT on Inferentia2\n",
    "\n",
    "Let's get started! ðŸš€\n",
    "\n",
    "---\n",
    "\n",
    "*If you are going to use Sagemaker in a local environment (not SageMaker Studio or Notebook Instances). You need access to an IAM Role with the required permissions for Sagemaker. You can findÂ [here](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html)Â more about it.*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e3db68e5",
   "metadata": {},
   "source": [
    "## 1. Convert BERT to AWS Neuron (Inferentia2) with `optimum-neuron`\n",
    "\n",
    "We are going to use the [optimum-neuron](https://huggingface.co/docs/optimum-neuron/index). ðŸ¤— Optimum Neuron is the interface between the ðŸ¤— Transformers library and AWS Accelerators including [AWS Trainium](https://aws.amazon.com/machine-learning/trainium/?nc1=h_ls) and [AWS Inferentia](https://aws.amazon.com/machine-learning/inferentia/?nc1=h_ls). It provides a set of tools enabling easy model loading, training and inference on single- and multi-Accelerator settings for different downstream tasks. \n",
    "\n",
    "As a first step, we need to install the `optimum-neuron` and other required packages.\n",
    "\n",
    "*Tip: If you are using Amazon SageMaker Notebook Instances or Studio you can go with the `conda_python3` conda kernel.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69c59d90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/huggingface/optimum-neuron.git@add-neuron-sd-pipe\n",
      "  Cloning https://github.com/huggingface/optimum-neuron.git (to revision add-neuron-sd-pipe) to /tmp/pip-req-build-hq0eh9iv\n",
      "  Running command git clone -q https://github.com/huggingface/optimum-neuron.git /tmp/pip-req-build-hq0eh9iv\n",
      "  Running command git checkout -b add-neuron-sd-pipe --track origin/add-neuron-sd-pipe\n",
      "  Switched to a new branch 'add-neuron-sd-pipe'\n",
      "  Branch 'add-neuron-sd-pipe' set up to track remote branch 'add-neuron-sd-pipe' from 'origin'.\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting diffusers\n",
      "  Using cached diffusers-0.18.2-py3-none-any.whl (1.2 MB)\n",
      "Requirement already satisfied, skipping upgrade: transformers>=4.28.0 in /usr/local/lib/python3.8/dist-packages (from optimum-neuron==0.0.8.dev0) (4.28.1)\n",
      "Requirement already satisfied, skipping upgrade: accelerate>=0.20.1 in /home/ubuntu/.local/lib/python3.8/site-packages (from optimum-neuron==0.0.8.dev0) (0.20.3)\n",
      "Requirement already satisfied, skipping upgrade: optimum>=1.8.8 in /home/ubuntu/.local/lib/python3.8/site-packages (from optimum-neuron==0.0.8.dev0) (1.8.8)\n",
      "Requirement already satisfied, skipping upgrade: huggingface-hub>=0.14.0 in /home/ubuntu/.local/lib/python3.8/site-packages (from optimum-neuron==0.0.8.dev0) (0.15.1)\n",
      "Requirement already satisfied, skipping upgrade: numpy<=1.21.6,>=1.18.5 in /usr/local/lib/python3.8/dist-packages (from optimum-neuron==0.0.8.dev0) (1.21.6)\n",
      "Requirement already satisfied, skipping upgrade: protobuf<4 in /usr/local/lib/python3.8/dist-packages (from optimum-neuron==0.0.8.dev0) (3.20.2)\n",
      "Requirement already satisfied, skipping upgrade: Pillow in /usr/local/lib/python3.8/dist-packages (from diffusers) (9.5.0)\n",
      "Requirement already satisfied, skipping upgrade: filelock in /usr/lib/python3/dist-packages (from diffusers) (3.0.12)\n",
      "Requirement already satisfied, skipping upgrade: importlib-metadata in /home/ubuntu/.local/lib/python3.8/site-packages (from diffusers) (4.13.0)\n",
      "Requirement already satisfied, skipping upgrade: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from diffusers) (2023.3.23)\n",
      "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.8/dist-packages (from diffusers) (2.28.2)\n",
      "Requirement already satisfied, skipping upgrade: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers>=4.28.0->optimum-neuron==0.0.8.dev0) (23.1)\n",
      "Requirement already satisfied, skipping upgrade: pyyaml>=5.1 in /home/ubuntu/.local/lib/python3.8/site-packages (from transformers>=4.28.0->optimum-neuron==0.0.8.dev0) (6.0)\n",
      "Requirement already satisfied, skipping upgrade: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers>=4.28.0->optimum-neuron==0.0.8.dev0) (0.13.3)\n",
      "Requirement already satisfied, skipping upgrade: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers>=4.28.0->optimum-neuron==0.0.8.dev0) (4.65.0)\n",
      "Requirement already satisfied, skipping upgrade: psutil in /usr/local/lib/python3.8/dist-packages (from accelerate>=0.20.1->optimum-neuron==0.0.8.dev0) (5.9.5)\n",
      "Requirement already satisfied, skipping upgrade: torch>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from accelerate>=0.20.1->optimum-neuron==0.0.8.dev0) (1.13.1)\n",
      "Requirement already satisfied, skipping upgrade: coloredlogs in /usr/local/lib/python3.8/dist-packages (from optimum>=1.8.8->optimum-neuron==0.0.8.dev0) (15.0.1)\n",
      "Requirement already satisfied, skipping upgrade: sympy in /usr/local/lib/python3.8/dist-packages (from optimum>=1.8.8->optimum-neuron==0.0.8.dev0) (1.11.1)\n",
      "Requirement already satisfied, skipping upgrade: torchvision in /usr/local/lib/python3.8/dist-packages (from optimum>=1.8.8->optimum-neuron==0.0.8.dev0) (0.14.1)\n",
      "Requirement already satisfied, skipping upgrade: datasets in /usr/local/lib/python3.8/dist-packages (from optimum>=1.8.8->optimum-neuron==0.0.8.dev0) (2.11.0)\n",
      "Requirement already satisfied, skipping upgrade: fsspec in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.14.0->optimum-neuron==0.0.8.dev0) (2023.4.0)\n",
      "Requirement already satisfied, skipping upgrade: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.14.0->optimum-neuron==0.0.8.dev0) (4.5.0)\n",
      "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/lib/python3/dist-packages (from importlib-metadata->diffusers) (1.0.0)\n",
      "Requirement already satisfied, skipping upgrade: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/dist-packages (from requests->diffusers) (3.1.0)\n",
      "Requirement already satisfied, skipping upgrade: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->diffusers) (2.8)\n",
      "Requirement already satisfied, skipping upgrade: urllib3<1.27,>=1.21.1 in /usr/lib/python3/dist-packages (from requests->diffusers) (1.25.8)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->diffusers) (2019.11.28)\n",
      "Requirement already satisfied, skipping upgrade: nvidia-cuda-runtime-cu11==11.7.99; platform_system == \"Linux\" in /usr/local/lib/python3.8/dist-packages (from torch>=1.6.0->accelerate>=0.20.1->optimum-neuron==0.0.8.dev0) (11.7.99)\n",
      "Requirement already satisfied, skipping upgrade: nvidia-cudnn-cu11==8.5.0.96; platform_system == \"Linux\" in /usr/local/lib/python3.8/dist-packages (from torch>=1.6.0->accelerate>=0.20.1->optimum-neuron==0.0.8.dev0) (8.5.0.96)\n",
      "Requirement already satisfied, skipping upgrade: nvidia-cublas-cu11==11.10.3.66; platform_system == \"Linux\" in /usr/local/lib/python3.8/dist-packages (from torch>=1.6.0->accelerate>=0.20.1->optimum-neuron==0.0.8.dev0) (11.10.3.66)\n",
      "Requirement already satisfied, skipping upgrade: nvidia-cuda-nvrtc-cu11==11.7.99; platform_system == \"Linux\" in /usr/local/lib/python3.8/dist-packages (from torch>=1.6.0->accelerate>=0.20.1->optimum-neuron==0.0.8.dev0) (11.7.99)\n",
      "Requirement already satisfied, skipping upgrade: humanfriendly>=9.1 in /usr/local/lib/python3.8/dist-packages (from coloredlogs->optimum>=1.8.8->optimum-neuron==0.0.8.dev0) (10.0)\n",
      "Requirement already satisfied, skipping upgrade: mpmath>=0.19 in /usr/local/lib/python3.8/dist-packages (from sympy->optimum>=1.8.8->optimum-neuron==0.0.8.dev0) (1.3.0)\n",
      "Requirement already satisfied, skipping upgrade: pyarrow>=8.0.0 in /usr/local/lib/python3.8/dist-packages (from datasets->optimum>=1.8.8->optimum-neuron==0.0.8.dev0) (11.0.0)\n",
      "Requirement already satisfied, skipping upgrade: dill<0.3.7,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from datasets->optimum>=1.8.8->optimum-neuron==0.0.8.dev0) (0.3.6)\n",
      "Requirement already satisfied, skipping upgrade: pandas in /usr/local/lib/python3.8/dist-packages (from datasets->optimum>=1.8.8->optimum-neuron==0.0.8.dev0) (2.0.0)\n",
      "Requirement already satisfied, skipping upgrade: xxhash in /usr/local/lib/python3.8/dist-packages (from datasets->optimum>=1.8.8->optimum-neuron==0.0.8.dev0) (3.2.0)\n",
      "Requirement already satisfied, skipping upgrade: multiprocess in /usr/local/lib/python3.8/dist-packages (from datasets->optimum>=1.8.8->optimum-neuron==0.0.8.dev0) (0.70.14)\n",
      "Requirement already satisfied, skipping upgrade: aiohttp in /usr/local/lib/python3.8/dist-packages (from datasets->optimum>=1.8.8->optimum-neuron==0.0.8.dev0) (3.8.4)\n",
      "Requirement already satisfied, skipping upgrade: responses<0.19 in /usr/local/lib/python3.8/dist-packages (from datasets->optimum>=1.8.8->optimum-neuron==0.0.8.dev0) (0.18.0)\n",
      "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.8/dist-packages (from nvidia-cuda-runtime-cu11==11.7.99; platform_system == \"Linux\"->torch>=1.6.0->accelerate>=0.20.1->optimum-neuron==0.0.8.dev0) (67.6.1)\n",
      "Requirement already satisfied, skipping upgrade: wheel in /usr/lib/python3/dist-packages (from nvidia-cuda-runtime-cu11==11.7.99; platform_system == \"Linux\"->torch>=1.6.0->accelerate>=0.20.1->optimum-neuron==0.0.8.dev0) (0.34.2)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil>=2.8.2 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets->optimum>=1.8.8->optimum-neuron==0.0.8.dev0) (2.8.2)\n",
      "Requirement already satisfied, skipping upgrade: pytz>=2020.1 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets->optimum>=1.8.8->optimum-neuron==0.0.8.dev0) (2023.3)\n",
      "Requirement already satisfied, skipping upgrade: tzdata>=2022.1 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets->optimum>=1.8.8->optimum-neuron==0.0.8.dev0) (2023.3)\n",
      "Requirement already satisfied, skipping upgrade: attrs>=17.3.0 in /home/ubuntu/.local/lib/python3.8/site-packages (from aiohttp->datasets->optimum>=1.8.8->optimum-neuron==0.0.8.dev0) (23.1.0)\n",
      "Requirement already satisfied, skipping upgrade: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->optimum>=1.8.8->optimum-neuron==0.0.8.dev0) (6.0.4)\n",
      "Requirement already satisfied, skipping upgrade: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->optimum>=1.8.8->optimum-neuron==0.0.8.dev0) (4.0.2)\n",
      "Requirement already satisfied, skipping upgrade: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->optimum>=1.8.8->optimum-neuron==0.0.8.dev0) (1.8.2)\n",
      "Requirement already satisfied, skipping upgrade: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->optimum>=1.8.8->optimum-neuron==0.0.8.dev0) (1.3.3)\n",
      "Requirement already satisfied, skipping upgrade: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->optimum>=1.8.8->optimum-neuron==0.0.8.dev0) (1.3.1)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->datasets->optimum>=1.8.8->optimum-neuron==0.0.8.dev0) (1.14.0)\n",
      "Building wheels for collected packages: optimum-neuron\n",
      "  Building wheel for optimum-neuron (PEP 517) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for optimum-neuron: filename=optimum_neuron-0.0.8.dev0-py3-none-any.whl size=165532 sha256=4bb13f3d5f94b05ab7670e66365dac5b8011e7865f38af786cd8a6e78ee467f4\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-9ix8ci_s/wheels/17/1b/7c/b5c3272ee099bec1c86c9ecab652c58e254e6a25dd33af943c\n",
      "Successfully built optimum-neuron\n",
      "Installing collected packages: diffusers, optimum-neuron\n",
      "  Attempting uninstall: diffusers\n",
      "    Found existing installation: diffusers 0.17.1\n",
      "    Uninstalling diffusers-0.17.1:\n",
      "      Successfully uninstalled diffusers-0.17.1\n",
      "  Attempting uninstall: optimum-neuron\n",
      "    Found existing installation: optimum-neuron 0.0.8.dev0\n",
      "    Uninstalling optimum-neuron-0.0.8.dev0:\n",
      "      Successfully uninstalled optimum-neuron-0.0.8.dev0\n",
      "Successfully installed diffusers-0.18.2 optimum-neuron-0.0.8.dev0\n",
      "Requirement already up-to-date: sagemaker==2.169.0 in /home/ubuntu/.local/lib/python3.8/site-packages (2.169.0)\n",
      "Requirement already satisfied, skipping upgrade: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from sagemaker==2.169.0) (23.1)\n",
      "Requirement already satisfied, skipping upgrade: pandas in /usr/local/lib/python3.8/dist-packages (from sagemaker==2.169.0) (2.0.0)\n",
      "Requirement already satisfied, skipping upgrade: boto3<2.0,>=1.26.131 in /home/ubuntu/.local/lib/python3.8/site-packages (from sagemaker==2.169.0) (1.26.161)\n",
      "Requirement already satisfied, skipping upgrade: attrs<24,>=23.1.0 in /home/ubuntu/.local/lib/python3.8/site-packages (from sagemaker==2.169.0) (23.1.0)\n",
      "Requirement already satisfied, skipping upgrade: protobuf3-to-dict<1.0,>=0.1.5 in /home/ubuntu/.local/lib/python3.8/site-packages (from sagemaker==2.169.0) (0.1.5)\n",
      "Requirement already satisfied, skipping upgrade: google-pasta in /home/ubuntu/.local/lib/python3.8/site-packages (from sagemaker==2.169.0) (0.2.0)\n",
      "Requirement already satisfied, skipping upgrade: tblib==1.7.0 in /home/ubuntu/.local/lib/python3.8/site-packages (from sagemaker==2.169.0) (1.7.0)\n",
      "Requirement already satisfied, skipping upgrade: smdebug-rulesconfig==1.0.1 in /home/ubuntu/.local/lib/python3.8/site-packages (from sagemaker==2.169.0) (1.0.1)\n",
      "Requirement already satisfied, skipping upgrade: PyYAML==6.0 in /home/ubuntu/.local/lib/python3.8/site-packages (from sagemaker==2.169.0) (6.0)\n",
      "Requirement already satisfied, skipping upgrade: importlib-metadata<5.0,>=1.4.0 in /home/ubuntu/.local/lib/python3.8/site-packages (from sagemaker==2.169.0) (4.13.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy<2.0,>=1.9.0 in /usr/local/lib/python3.8/dist-packages (from sagemaker==2.169.0) (1.21.6)\n",
      "Requirement already satisfied, skipping upgrade: protobuf<4.0,>=3.1 in /usr/local/lib/python3.8/dist-packages (from sagemaker==2.169.0) (3.20.2)\n",
      "Requirement already satisfied, skipping upgrade: platformdirs in /usr/local/lib/python3.8/dist-packages (from sagemaker==2.169.0) (3.2.0)\n",
      "Requirement already satisfied, skipping upgrade: pathos in /home/ubuntu/.local/lib/python3.8/site-packages (from sagemaker==2.169.0) (0.3.0)\n",
      "Requirement already satisfied, skipping upgrade: schema in /home/ubuntu/.local/lib/python3.8/site-packages (from sagemaker==2.169.0) (0.7.5)\n",
      "Requirement already satisfied, skipping upgrade: cloudpickle==2.2.1 in /home/ubuntu/.local/lib/python3.8/site-packages (from sagemaker==2.169.0) (2.2.1)\n",
      "Requirement already satisfied, skipping upgrade: jsonschema in /usr/lib/python3/dist-packages (from sagemaker==2.169.0) (3.2.0)\n",
      "Requirement already satisfied, skipping upgrade: pytz>=2020.1 in /usr/local/lib/python3.8/dist-packages (from pandas->sagemaker==2.169.0) (2023.3)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil>=2.8.2 in /usr/local/lib/python3.8/dist-packages (from pandas->sagemaker==2.169.0) (2.8.2)\n",
      "Requirement already satisfied, skipping upgrade: tzdata>=2022.1 in /usr/local/lib/python3.8/dist-packages (from pandas->sagemaker==2.169.0) (2023.3)\n",
      "Requirement already satisfied, skipping upgrade: jmespath<2.0.0,>=0.7.1 in /home/ubuntu/.local/lib/python3.8/site-packages (from boto3<2.0,>=1.26.131->sagemaker==2.169.0) (1.0.1)\n",
      "Requirement already satisfied, skipping upgrade: botocore<1.30.0,>=1.29.161 in /home/ubuntu/.local/lib/python3.8/site-packages (from boto3<2.0,>=1.26.131->sagemaker==2.169.0) (1.29.161)\n",
      "Requirement already satisfied, skipping upgrade: s3transfer<0.7.0,>=0.6.0 in /home/ubuntu/.local/lib/python3.8/site-packages (from boto3<2.0,>=1.26.131->sagemaker==2.169.0) (0.6.1)\n",
      "Requirement already satisfied, skipping upgrade: six in /usr/lib/python3/dist-packages (from protobuf3-to-dict<1.0,>=0.1.5->sagemaker==2.169.0) (1.14.0)\n",
      "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/lib/python3/dist-packages (from importlib-metadata<5.0,>=1.4.0->sagemaker==2.169.0) (1.0.0)\n",
      "Requirement already satisfied, skipping upgrade: dill>=0.3.6 in /usr/local/lib/python3.8/dist-packages (from pathos->sagemaker==2.169.0) (0.3.6)\n",
      "Requirement already satisfied, skipping upgrade: multiprocess>=0.70.14 in /usr/local/lib/python3.8/dist-packages (from pathos->sagemaker==2.169.0) (0.70.14)\n",
      "Requirement already satisfied, skipping upgrade: ppft>=1.7.6.6 in /home/ubuntu/.local/lib/python3.8/site-packages (from pathos->sagemaker==2.169.0) (1.7.6.6)\n",
      "Requirement already satisfied, skipping upgrade: pox>=0.3.2 in /home/ubuntu/.local/lib/python3.8/site-packages (from pathos->sagemaker==2.169.0) (0.3.2)\n",
      "Requirement already satisfied, skipping upgrade: contextlib2>=0.5.5 in /home/ubuntu/.local/lib/python3.8/site-packages (from schema->sagemaker==2.169.0) (21.6.0)\n",
      "Requirement already satisfied, skipping upgrade: urllib3<1.27,>=1.25.4 in /usr/lib/python3/dist-packages (from botocore<1.30.0,>=1.29.161->boto3<2.0,>=1.26.131->sagemaker==2.169.0) (1.25.8)\n"
     ]
    }
   ],
   "source": [
    "# Install the required packages\n",
    "# !pip install \"optimum-neuron[neuronx]==0.0.6\"  --upgrade\n",
    "!pip install \"git+https://github.com/huggingface/optimum-neuron.git@add-neuron-sd-pipe\" diffusers --upgrade\n",
    "\n",
    "!python -m pip install \"sagemaker==2.169.0\"  --upgrade"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ce0ef431",
   "metadata": {},
   "source": [
    "After we have installed the `optimum-neuron` we can convert load and convert our model.\n",
    "\n",
    "We are going to use the [yiyanghkust/finbert-tone](https://huggingface.co/yiyanghkust/finbert-tone) model. FinBERT is a BERT model pre-trained on financial communication text. The purpose is to enhance financial NLP research and practice. It is trained on the following three financial communication corpus. The total corpora size is 4.9B tokens. This released finbert-tone model is the FinBERT model fine-tuned on 10,000 manually annotated (positive, negative, neutral) sentences from analyst reports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96d8dfea",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"runwayml/stable-diffusion-v1-5\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9e4386d9",
   "metadata": {},
   "source": [
    "At the time of writing, the [AWS Inferentia2 does not support dynamic shapes for inference](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/arch/neuron-features/dynamic-shapes.html?highlight=dynamic%20shapes#), which means that the input size needs to be static for compiling and inference. \n",
    "\n",
    "In simpler terms, this means when the model is converted with a sequence length of 16. The model can only run inference on inputs with the same shape. We are going to use the `optimum-cli` to convert our model with a sequence length of 128 and a batch size of 1. \n",
    "\n",
    "You need to use ~50GB of memory and the compilation can take 5-10 minutes_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4de3d58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (â€¦)ain/model_index.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 541/541 [00:00<00:00, 213kB/s]\n",
      "Downloading (â€¦)cheduler_config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 308/308 [00:00<00:00, 78.5kB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "Downloading (â€¦)_checker/config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4.72k/4.72k [00:00<00:00, 292kB/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Downloading (â€¦)rocessor_config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 342/342 [00:00<00:00, 21.9kB/s]\n",
      "Downloading (â€¦)_encoder/config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 617/617 [00:00<00:00, 30.8kB/s]\n",
      "Downloading (â€¦)cial_tokens_map.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 472/472 [00:00<00:00, 135kB/s]\n",
      "Downloading (â€¦)okenizer_config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 806/806 [00:00<00:00, 1.00MB/s]\n",
      "Fetching 15 files:  20%|â–ˆâ–ˆ        | 3/15 [00:00<00:00, 30.51it/s]\n",
      "Downloading (â€¦)7f0/unet/config.json:   0%|          | 0.00/743 [00:00<?, ?B/s]\n",
      "Downloading (â€¦)7f0/unet/config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 743/743 [00:00<00:00, 116kB/s]\n",
      "Downloading (â€¦)tokenizer/merges.txt:   0%|          | 0.00/525k [00:00<?, ?B/s]\n",
      "\n",
      "Downloading (â€¦)57f0/vae/config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 547/547 [00:00<00:00, 664kB/s]\n",
      "\n",
      "\n",
      "Downloading (â€¦)tokenizer/vocab.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.06M/1.06M [00:00<00:00, 30.5MB/s]\n",
      "Downloading (â€¦)tokenizer/merges.txt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 525k/525k [00:00<00:00, 28.8MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Downloading pytorch_model.bin: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 492M/492M [00:01<00:00, 481MB/s]\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Downloading (â€¦)on_pytorch_model.bin: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.44G/3.44G [00:06<00:00, 535MB/s]\n",
      "Loading TensorFlow model in PyTorch before exporting.\n",
      "Fetching 15 files:   0%|          | 0/15 [00:00<?, ?it/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Downloading (â€¦)on_pytorch_model.bin: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 335M/335M [00:00<00:00, 547MB/s]\n",
      "Downloading pytorch_model.bin: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.22G/1.22G [00:02<00:00, 546MB/s]\n",
      "Fetching 15 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:02<00:00,  6.69it/s]\n",
      "Keyword arguments {'subfolder': '', 'trust_remote_code': False, 'from_tf': True} are not expected by StableDiffusionPipeline and will be ignored.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
      "Downloading pytorch_model.bin:   0%|          | 0.00/1.22G [00:12<?, ?B/s]\n",
      "Downloading (â€¦)on_pytorch_model.bin:   6%|â–‹         | 21.0M/335M [00:12<03:01, 1.73MB/s]\n",
      "usage: neuronx-cc compile --framework {XLA} --target {trn1,inf2,trn1n}\n",
      "                          [--enable-fast-loading-neuron-binaries]\n",
      "                          [--enable-fast-context-switch]\n",
      "                          [--auto-cast <cast mode>]\n",
      "                          [--auto-cast-type {fp16,bf16,tf32,fp8_e4m3}]\n",
      "                          [--output <filename>] [--logfile <filename>]\n",
      "                          [--help]\n",
      "                          [--model-type {transformer,transformer-inference,generic}]\n",
      "                          [--verbose {debug|info|warning|error|critical}]\n",
      "neuronx-cc compile: error: argument --model-type: invalid choice: unet-inference (choose from transformer, transformer-inference, generic)\n",
      "An error occured when trying to trace unet with the error message: neuronx-cc failed with 2.\n",
      "The export is failed and unet neuron model won't be stored.\n",
      "2023-07-17T06:44:45Z WARNING 8282 [SB_Allocator]: accumulation group is too large for SB\n",
      "2023-07-17T06:50:51Z ERROR 9513 [neuronx-cc]: ***************************************************************\n",
      "2023-07-17T06:50:51Z ERROR 9513 [neuronx-cc]:  An Internal Compiler Error has occurred\n",
      "2023-07-17T06:50:51Z ERROR 9513 [neuronx-cc]: ***************************************************************\n",
      "2023-07-17T06:50:51Z ERROR 9513 [neuronx-cc]: \n",
      "2023-07-17T06:50:51Z ERROR 9513 [neuronx-cc]: Error message:  Too many instructions after unroll!\n",
      "2023-07-17T06:50:51Z ERROR 9513 [neuronx-cc]: \n",
      "2023-07-17T06:50:51Z ERROR 9513 [neuronx-cc]: Error class:    AssertionError\n",
      "2023-07-17T06:50:51Z ERROR 9513 [neuronx-cc]: Error location: Unknown\n",
      "2023-07-17T06:50:51Z ERROR 9513 [neuronx-cc]: Command line:   /usr/local/bin/neuronx-cc compile /tmp/tmpffh0q4lo/model --framework XLA --target trn1 --output /tmp/tmpffh0q4lo/graph.neff --auto-cast none\n",
      "2023-07-17T06:50:51Z ERROR 9513 [neuronx-cc]: \n",
      "2023-07-17T06:50:51Z ERROR 9513 [neuronx-cc]: Internal details:\n",
      "2023-07-17T06:50:51Z ERROR 9513 [neuronx-cc]:   File \"neuronxcc/driver/CommandDriver.py\", line 237, in neuronxcc.driver.CommandDriver.CommandDriver.run\n",
      "2023-07-17T06:50:51Z ERROR 9513 [neuronx-cc]:   File \"neuronxcc/driver/commands/CompileCommand.py\", line 1047, in neuronxcc.driver.commands.CompileCommand.CompileCommand.run\n",
      "2023-07-17T06:50:51Z ERROR 9513 [neuronx-cc]:   File \"neuronxcc/driver/commands/CompileCommand.py\", line 998, in neuronxcc.driver.commands.CompileCommand.CompileCommand.runPipeline\n",
      "2023-07-17T06:50:51Z ERROR 9513 [neuronx-cc]:   File \"neuronxcc/driver/commands/CompileCommand.py\", line 1023, in neuronxcc.driver.commands.CompileCommand.CompileCommand.runPipeline\n",
      "2023-07-17T06:50:51Z ERROR 9513 [neuronx-cc]:   File \"neuronxcc/driver/commands/CompileCommand.py\", line 1027, in neuronxcc.driver.commands.CompileCommand.CompileCommand.runPipeline\n",
      "2023-07-17T06:50:51Z ERROR 9513 [neuronx-cc]:   File \"neuronxcc/driver/Job.py\", line 300, in neuronxcc.driver.Job.SingleInputJob.run\n",
      "2023-07-17T06:50:51Z ERROR 9513 [neuronx-cc]:   File \"neuronxcc/driver/Job.py\", line 326, in neuronxcc.driver.Job.SingleInputJob.runOnState\n",
      "2023-07-17T06:50:51Z ERROR 9513 [neuronx-cc]:   File \"neuronxcc/driver/Pipeline.py\", line 30, in neuronxcc.driver.Pipeline.Pipeline.runSingleInput\n",
      "2023-07-17T06:50:51Z ERROR 9513 [neuronx-cc]:   File \"neuronxcc/driver/Job.py\", line 300, in neuronxcc.driver.Job.SingleInputJob.run\n",
      "2023-07-17T06:50:51Z ERROR 9513 [neuronx-cc]:   File \"neuronxcc/driver/Job.py\", line 326, in neuronxcc.driver.Job.SingleInputJob.runOnState\n",
      "2023-07-17T06:50:51Z ERROR 9513 [neuronx-cc]:   File \"neuronxcc/driver/jobs/Frontend.py\", line 343, in neuronxcc.driver.jobs.Frontend.Frontend.runSingleInput\n",
      "2023-07-17T06:50:51Z ERROR 9513 [neuronx-cc]:   File \"neuronxcc/driver/jobs/Frontend.py\", line 148, in neuronxcc.driver.jobs.Frontend.Frontend.runXLAFrontend\n",
      "2023-07-17T06:50:51Z ERROR 9513 [neuronx-cc]:   File \"neuronxcc/starfish/penguin/Penguin.py\", line 297, in neuronxcc.starfish.penguin.Penguin.runPenguin\n",
      "2023-07-17T06:50:51Z ERROR 9513 [neuronx-cc]:   File \"neuronxcc/starfish/penguin/Frontend.py\", line 150, in neuronxcc.starfish.penguin.Frontend.tensorizeXla\n",
      "2023-07-17T06:50:51Z ERROR 9513 [neuronx-cc]:   File \"neuronxcc/starfish/penguin/Frontend.py\", line 151, in neuronxcc.starfish.penguin.Frontend.tensorizeXla\n",
      "2023-07-17T06:50:51Z ERROR 9513 [neuronx-cc]:   File \"neuronxcc/starfish/penguin/Frontend.py\", line 159, in neuronxcc.starfish.penguin.Frontend.tensorizeXla\n",
      "2023-07-17T06:50:51Z ERROR 9513 [neuronx-cc]:   File \"neuronxcc/starfish/penguin/Frontend.py\", line 212, in neuronxcc.starfish.penguin.Frontend.tensorizeXlaFromFile\n",
      "2023-07-17T06:50:51Z ERROR 9513 [neuronx-cc]:   File \"neuronxcc/starfish/penguin/Compile.py\", line 215, in neuronxcc.starfish.penguin.Compile.compile_module\n",
      "2023-07-17T06:50:51Z ERROR 9513 [neuronx-cc]:   File \"neuronxcc/starfish/penguin/Compile.py\", line 217, in neuronxcc.starfish.penguin.Compile.compile_module\n",
      "2023-07-17T06:50:51Z ERROR 9513 [neuronx-cc]:   File \"neuronxcc/starfish/penguin/Compile.py\", line 257, in neuronxcc.starfish.penguin.Compile.compile_module\n",
      "2023-07-17T06:50:51Z ERROR 9513 [neuronx-cc]:   File \"neuronxcc/starfish/penguin/DotTransform.py\", line 445, in neuronxcc.starfish.penguin.DotTransform.PassManager.transformModule\n",
      "2023-07-17T06:50:51Z ERROR 9513 [neuronx-cc]:   File \"neuronxcc/starfish/penguin/DotTransform.py\", line 464, in neuronxcc.starfish.penguin.DotTransform.PassManager.transformFunction\n",
      "2023-07-17T06:50:51Z ERROR 9513 [neuronx-cc]:   File \"neuronxcc/starfish/penguin/DotTransform.py\", line 471, in neuronxcc.starfish.penguin.DotTransform.PassManager.transformFunction\n",
      "2023-07-17T06:50:51Z ERROR 9513 [neuronx-cc]:   File \"neuronxcc/starfish/penguin/DotTransform.py\", line 143, in neuronxcc.starfish.penguin.DotTransform.DotTransform.runOnFunction\n",
      "2023-07-17T06:50:51Z ERROR 9513 [neuronx-cc]:   File \"neuronxcc/starfish/penguin/DotTransform.py\", line 196, in neuronxcc.starfish.penguin.DotTransform.DotTransform.run_with_exception_handling\n",
      "2023-07-17T06:50:51Z ERROR 9513 [neuronx-cc]:   File \"neuronxcc/starfish/penguin/DotTransform.py\", line 191, in neuronxcc.starfish.penguin.DotTransform.DotTransform.run_with_exception_handling\n",
      "2023-07-17T06:50:51Z ERROR 9513 [neuronx-cc]:   File \"neuronxcc/starfish/penguin/DotTransform.py\", line 208, in neuronxcc.starfish.penguin.DotTransform.DotTransform.timed_run_\n",
      "2023-07-17T06:50:51Z ERROR 9513 [neuronx-cc]:   File \"neuronxcc/starfish/penguin/DotTransform.py\", line 210, in neuronxcc.starfish.penguin.DotTransform.DotTransform.timed_run_\n",
      "2023-07-17T06:50:51Z ERROR 9513 [neuronx-cc]:   File \"neuronxcc/starfish/penguin/DotTransform.py\", line 211, in neuronxcc.starfish.penguin.DotTransform.DotTransform.timed_run_\n",
      "2023-07-17T06:50:51Z ERROR 9513 [neuronx-cc]:   File \"neuronxcc/starfish/penguin/DotTransform.py\", line 240, in neuronxcc.starfish.penguin.DotTransform.DotTransform.run_\n",
      "2023-07-17T06:50:51Z ERROR 9513 [neuronx-cc]:   File \"neuronxcc/starfish/penguin/DotTransform.py\", line 242, in neuronxcc.starfish.penguin.DotTransform.DotTransform.run_\n",
      "2023-07-17T06:50:51Z ERROR 9513 [neuronx-cc]:   File \"neuronxcc/starfish/penguin/DotTransform.py\", line 342, in neuronxcc.starfish.penguin.DotTransform.DotTransform.transformFunction\n",
      "2023-07-17T06:50:51Z ERROR 9513 [neuronx-cc]:   File \"neuronxcc/starfish/penguin/DotTransform.py\", line 343, in neuronxcc.starfish.penguin.DotTransform.DotTransform.transformFunction\n",
      "2023-07-17T06:50:51Z ERROR 9513 [neuronx-cc]:   File \"neuronxcc/starfish/penguin/DotTransform.py\", line 334, in neuronxcc.starfish.penguin.DotTransform.DotTransform.runTransforms\n",
      "2023-07-17T06:50:51Z ERROR 9513 [neuronx-cc]:   File \"neuronxcc/starfish/penguin/targets/sunda/passes/SundaSizeTiling.py\", line 2136, in neuronxcc.starfish.penguin.targets.sunda.passes.SundaSizeTiling.SundaSizeTiling.afterStmtTransform\n",
      "2023-07-17T06:50:51Z ERROR 9513 [neuronx-cc]: \n",
      "2023-07-17T06:50:51Z ERROR 9513 [neuronx-cc]: Version information:\n",
      "2023-07-17T06:50:51Z ERROR 9513 [neuronx-cc]:   NeuronX Compiler version 2.5.0.28+1be23f232\n",
      "2023-07-17T06:50:51Z ERROR 9513 [neuronx-cc]:   \n",
      "2023-07-17T06:50:51Z ERROR 9513 [neuronx-cc]:   HWM version 2.5.0.0-dad732dd6\n",
      "2023-07-17T06:50:51Z ERROR 9513 [neuronx-cc]:   NEFF version Dynamic\n",
      "2023-07-17T06:50:51Z ERROR 9513 [neuronx-cc]:   TVM not available\n",
      "2023-07-17T06:50:51Z ERROR 9513 [neuronx-cc]:   NumPy version 1.21.6\n",
      "2023-07-17T06:50:51Z ERROR 9513 [neuronx-cc]:   MXNet not available\n",
      "2023-07-17T06:50:51Z ERROR 9513 [neuronx-cc]: \n",
      "2023-07-17T06:50:51Z ERROR 9513 [neuronx-cc]: Artifacts stored in: /home/ubuntu/huggingface-inferentia2-samples/stable-diffusion/neuronxcc-eieykkc6\n",
      "An error occured when trying to trace vae_decoder with the error message: neuronx-cc failed with 1.\n",
      "The export is failed and vae_decoder neuron model won't be stored.\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39moptimum\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mneuron\u001b[39;00m \u001b[39mimport\u001b[39;00m NeuronStableDiffusionPipeline\n\u001b[1;32m      3\u001b[0m input_shapes \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mbatch_size\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m1\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39msequence_length\u001b[39m\u001b[39m\"\u001b[39m:\u001b[39m64\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mnum_channels\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m4\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mheight\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m512\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mwidth\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m512\u001b[39m}\n\u001b[0;32m----> 5\u001b[0m stable_diffusion \u001b[39m=\u001b[39m NeuronStableDiffusionPipeline\u001b[39m.\u001b[39;49mfrom_pretrained(model_id, export\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minput_shapes)\n\u001b[1;32m      7\u001b[0m \u001b[39m# Save locally or upload to the HuggingFace Hub\u001b[39;00m\n\u001b[1;32m      8\u001b[0m save_directory \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39msd_neuron/\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/optimum/modeling_base.py:367\u001b[0m, in \u001b[0;36mOptimizedModel.from_pretrained\u001b[0;34m(cls, model_id, export, force_download, use_auth_token, cache_dir, subfolder, config, local_files_only, trust_remote_code, revision, **kwargs)\u001b[0m\n\u001b[1;32m    364\u001b[0m     trust_remote_code \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    366\u001b[0m from_pretrained_method \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_from_transformers \u001b[39mif\u001b[39;00m export \u001b[39melse\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_from_pretrained\n\u001b[0;32m--> 367\u001b[0m \u001b[39mreturn\u001b[39;00m from_pretrained_method(\n\u001b[1;32m    368\u001b[0m     model_id\u001b[39m=\u001b[39;49mmodel_id,\n\u001b[1;32m    369\u001b[0m     config\u001b[39m=\u001b[39;49mconfig,\n\u001b[1;32m    370\u001b[0m     revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m    371\u001b[0m     cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m    372\u001b[0m     force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m    373\u001b[0m     use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[1;32m    374\u001b[0m     subfolder\u001b[39m=\u001b[39;49msubfolder,\n\u001b[1;32m    375\u001b[0m     local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m    376\u001b[0m     trust_remote_code\u001b[39m=\u001b[39;49mtrust_remote_code,\n\u001b[1;32m    377\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    378\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/optimum/neuron/modeling_diffusion.py:371\u001b[0m, in \u001b[0;36mNeuronStableDiffusionPipelineBase._from_transformers\u001b[0;34m(cls, model_id, config, use_auth_token, revision, force_download, cache_dir, subfolder, local_files_only, trust_remote_code, task, auto_cast, auto_cast_type, disable_fast_relayout, disable_fallback, dynamic_batch_size, **kwargs_shapes)\u001b[0m\n\u001b[1;32m    368\u001b[0m save_dir \u001b[39m=\u001b[39m TemporaryDirectory()\n\u001b[1;32m    369\u001b[0m save_dir_path \u001b[39m=\u001b[39m Path(save_dir\u001b[39m.\u001b[39mname)\n\u001b[0;32m--> 371\u001b[0m main_export(\n\u001b[1;32m    372\u001b[0m     model_name_or_path\u001b[39m=\u001b[39;49mmodel_id,\n\u001b[1;32m    373\u001b[0m     output\u001b[39m=\u001b[39;49msave_dir_path,\n\u001b[1;32m    374\u001b[0m     compiler_kwargs\u001b[39m=\u001b[39;49mcompiler_kwargs,\n\u001b[1;32m    375\u001b[0m     task\u001b[39m=\u001b[39;49mtask,\n\u001b[1;32m    376\u001b[0m     dynamic_batch_size\u001b[39m=\u001b[39;49mdynamic_batch_size,\n\u001b[1;32m    377\u001b[0m     cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m    378\u001b[0m     trust_remote_code\u001b[39m=\u001b[39;49mtrust_remote_code,\n\u001b[1;32m    379\u001b[0m     subfolder\u001b[39m=\u001b[39;49msubfolder,\n\u001b[1;32m    380\u001b[0m     revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m    381\u001b[0m     force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m    382\u001b[0m     local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m    383\u001b[0m     use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[1;32m    384\u001b[0m     do_validation\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    385\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minput_shapes,\n\u001b[1;32m    386\u001b[0m )\n\u001b[1;32m    388\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_from_pretrained(\n\u001b[1;32m    389\u001b[0m     model_id\u001b[39m=\u001b[39msave_dir_path,\n\u001b[1;32m    390\u001b[0m     config\u001b[39m=\u001b[39mconfig,\n\u001b[1;32m    391\u001b[0m     model_save_dir\u001b[39m=\u001b[39msave_dir,\n\u001b[1;32m    392\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/optimum/exporters/neuron/__main__.py:192\u001b[0m, in \u001b[0;36mmain_export\u001b[0;34m(model_name_or_path, output, compiler_kwargs, task, dynamic_batch_size, atol, cache_dir, trust_remote_code, subfolder, revision, force_download, local_files_only, use_auth_token, do_validation, **input_shapes)\u001b[0m\n\u001b[1;32m    189\u001b[0m         model\u001b[39m.\u001b[39mfeature_extractor\u001b[39m.\u001b[39msave_pretrained(output\u001b[39m.\u001b[39mjoinpath(\u001b[39m\"\u001b[39m\u001b[39mfeature_extractor\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m    190\u001b[0m     model\u001b[39m.\u001b[39msave_config(output)\n\u001b[0;32m--> 192\u001b[0m _, neuron_outputs \u001b[39m=\u001b[39m export_models(\n\u001b[1;32m    193\u001b[0m     models_and_neuron_configs\u001b[39m=\u001b[39;49mmodels_and_neuron_configs,\n\u001b[1;32m    194\u001b[0m     output_dir\u001b[39m=\u001b[39;49moutput,\n\u001b[1;32m    195\u001b[0m     output_file_names\u001b[39m=\u001b[39;49moutput_model_names,\n\u001b[1;32m    196\u001b[0m     compiler_kwargs\u001b[39m=\u001b[39;49mcompiler_kwargs,\n\u001b[1;32m    197\u001b[0m )\n\u001b[1;32m    199\u001b[0m \u001b[39mdel\u001b[39;00m model\n\u001b[1;32m    201\u001b[0m \u001b[39m# Validate compiled model\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/optimum/exporters/neuron/convert.py:343\u001b[0m, in \u001b[0;36mexport_models\u001b[0;34m(models_and_neuron_configs, output_dir, output_file_names, compiler_kwargs, configs)\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[39m# remove models failed to export\u001b[39;00m\n\u001b[1;32m    342\u001b[0m \u001b[39mfor\u001b[39;00m i, model_name \u001b[39min\u001b[39;00m failed_models:\n\u001b[0;32m--> 343\u001b[0m     output_file_names\u001b[39m.\u001b[39;49mpop(i)\n\u001b[1;32m    344\u001b[0m     models_and_neuron_configs\u001b[39m.\u001b[39mpop(model_name)\n\u001b[1;32m    346\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mmap\u001b[39m(\u001b[39mlist\u001b[39m, \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39moutputs)))\n",
      "\u001b[0;31mKeyError\u001b[0m: 1"
     ]
    }
   ],
   "source": [
    "from optimum.neuron import NeuronStableDiffusionPipeline\n",
    "\n",
    "input_shapes = {\"batch_size\": 1,\"sequence_length\":64, \"height\": 512, \"width\": 512}\n",
    "\n",
    "stable_diffusion = NeuronStableDiffusionPipeline.from_pretrained(model_id, export=True, **input_shapes)\n",
    "\n",
    "# Save locally or upload to the HuggingFace Hub\n",
    "save_directory = \"sd_neuron/\"\n",
    "stable_diffusion.save_pretrained(save_directory)\n",
    "\n",
    "# COMMENT IN TO UPLOAD TO HUB\n",
    "# stable_diffusion.push_to_hub(\n",
    "#      save_directory, repository_id=\"my-neuron-repo\", use_auth_token=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44ff097",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$model_id\"\n",
    "MODEL_ID=$1\n",
    "SEQUENCE_LENGTH=128\n",
    "BATCH_SIZE=1\n",
    "OUTPUT_DIR=tmp/ # used to store temproary files\n",
    "echo \"Model ID: $MODEL_ID\"\n",
    "\n",
    "# exporting model\n",
    "optimum-cli export neuron \\\n",
    "  --model $MODEL_ID \\\n",
    "  --sequence_length $SEQUENCE_LENGTH \\\n",
    "  --batch_size $BATCH_SIZE \\\n",
    "  $OUTPUT_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710b3787",
   "metadata": {},
   "outputs": [],
   "source": [
    "        self.pipeline = StableDiffusionPipeline.from_pretrained(model_dir, torch_dtype=torch.float16)\n",
    "        # try to use DPMSolverMultistepScheduler\n",
    "        try:\n",
    "            self.pipeline.scheduler = DPMSolverMultistepScheduler.from_config(self.pipeline.scheduler.config)\n",
    "        except Exception:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9997e9db",
   "metadata": {},
   "source": [
    "## 2. Create a custom `inference.py` script for `text-classification`\n",
    "\n",
    "The [Hugging Face Inference Toolkit](https://github.com/aws/sagemaker-huggingface-inference-toolkit) supports zero-code deployments on top of theÂ [pipelineÂ feature](https://huggingface.co/transformers/main_classes/pipelines.html)Â from ðŸ¤— Transformers. This allows users to deploy Hugging Face transformers without an inference script [[Example](https://github.com/huggingface/notebooks/blob/master/sagemaker/11_deploy_model_from_hf_hub/deploy_transformer_model_from_hf_hub.ipynb)]. \n",
    "\n",
    "Currently is this feature not supported with AWS Inferentia2, which means we need to provide an `inference.py` for running inference. But `optimum-neuron` has integrated support for the ðŸ¤— Transformers pipeline feature. That way we can use the `optimum-neuron` to create a pipeline for our model.\n",
    "\n",
    "If you want to know more about the `inference.py`Â script check out this [example](https://github.com/huggingface/notebooks/blob/master/sagemaker/17_custom_inference_script/sagemaker-notebook.ipynb). It explains amongst other things what the `model_fn` and `predict_fn` are. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4246c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir code"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "39f74625",
   "metadata": {},
   "source": [
    "In addition to our `inference.py` script we need to provide a `requirements.txt`, which installs the latest version of the `optimum-neuron` package, which comes with `pipeline` support for AWS Inferentia2. \n",
    "_Note: This is a temporary solution until the `optimum-neuron` package is updated inside the DLC._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09fd8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile code/requirements.txt\n",
    "git+https://github.com/huggingface/optimum-neuron.git@b94d534cc0160f1e199fae6ae3a1c7b804b49e30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce675df9",
   "metadata": {},
   "source": [
    "We are using the `NEURON_RT_NUM_CORES=1` to make sure that each HTTP worker uses 1 Neuron core to maximize throughput."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce41529",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile code/inference.py\n",
    "import os\n",
    "# To use one neuron core per worker\n",
    "os.environ[\"NEURON_RT_NUM_CORES\"] = \"1\"\n",
    "from optimum.neuron.pipelines import pipeline\n",
    "import torch\n",
    "import torch_neuronx\n",
    "\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    # load local converted model into pipeline\n",
    "    pipe = pipeline(\"text-classification\", model=model_dir)\n",
    "    return pipe\n",
    "\n",
    "\n",
    "def predict_fn(data, pipeline):\n",
    "    inputs = data.pop(\"inputs\", data)\n",
    "    parameters = data.pop(\"parameters\", None)\n",
    "\n",
    "    # pass inputs with all kwargs in data\n",
    "    if parameters is not None:\n",
    "        prediction = pipeline(inputs, **parameters)\n",
    "    else:\n",
    "        prediction = pipeline(inputs)\n",
    "    # postprocess the prediction\n",
    "    return prediction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "144d8ccb",
   "metadata": {},
   "source": [
    "## 3. Upload the neuron model and inference script to Amazon S3\n",
    "\n",
    "Before we can deploy our neuron model to Amazon SageMaker we need to create a `model.tar.gz` archive with all our model artifacts saved into, e.g.Â `model.neuron` and upload this to Amazon S3.\n",
    "\n",
    "To do this we need to set up our permissions. Currently `inf2` instances are only available in the `us-east-2` region [[REF](https://aws.amazon.com/de/about-aws/whats-new/2023/05/sagemaker-ml-inf2-ml-trn1-instances-model-deployment/)]. Therefore we need to force the region to us-east-2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d016feac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "os.environ[\"AWS_DEFAULT_REGION\"] = \"us-east-2\" # need to set to ohio region"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "83ea3df1",
   "metadata": {},
   "source": [
    "Now lets create our SageMaker session and upload our model to Amazon S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952983b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")\n",
    "assert sess.boto_region_name == \"us-east-2\", \"region must be us-east-2\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374ff630",
   "metadata": {},
   "source": [
    "Next, we create our `model.tar.gz`.TheÂ `inference.py`Â script will be placed into aÂ `code/`Â folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3808b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy inference.py into the code/ directory of the model directory.\n",
    "!cp -r code/ tmp/code/\n",
    "# create a model.tar.gz archive with all the model artifacts and the inference.py script.\n",
    "%cd tmp\n",
    "!tar zcvf model.tar.gz *\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a6f330",
   "metadata": {},
   "source": [
    "Now we can upload our `model.tar.gz` to our session S3 bucket with `sagemaker`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6146af09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.s3 import S3Uploader\n",
    "\n",
    "# create s3 uri\n",
    "s3_model_path = f\"s3://{sess.default_bucket()}/neuronx/{model_id}\"\n",
    "\n",
    "# upload model.tar.gz\n",
    "s3_model_uri = S3Uploader.upload(local_path=\"tmp/model.tar.gz\",desired_s3_uri=s3_model_path)\n",
    "print(f\"model artifcats uploaded to {s3_model_uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cccbb719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean tmp directory after uploading\n",
    "# !rm -rf tmp"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "04e1395a",
   "metadata": {},
   "source": [
    "## 4. Deploy a Real-time Inference Endpoint on Amazon SageMaker\n",
    "\n",
    "After we have uploaded ourÂ `model.tar.gz`Â to Amazon S3 can we create a customÂ `HuggingfaceModel`. This class will be used to create and deploy our real-time inference endpoint on Amazon SageMaker.\n",
    "\n",
    "The `inf2.xlarge` instance type is the smallest instance type with AWS Inferentia2 support. It comes with 1 Inferentia2 chip with 2 Neuron Cores. This means we can use 2 Model server workers to maximize throughput and run 2 inferences in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41522ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface.model import HuggingFaceModel\n",
    "\n",
    "# create Hugging Face Model Class\n",
    "huggingface_model = HuggingFaceModel(\n",
    "   model_data=s3_model_uri,        # path to your model and script\n",
    "   role=role,                      # iam role with permissions to create an Endpoint\n",
    "   transformers_version=\"4.28.1\",  # transformers version used\n",
    "   pytorch_version=\"1.13.0\",       # pytorch version used\n",
    "   py_version='py38',              # python version used\n",
    "   model_server_workers=2,         # number of workers for the model server\n",
    ")\n",
    "\n",
    "# Let SageMaker know that we've already compiled the model\n",
    "huggingface_model._is_compiled_model = True\n",
    "\n",
    "# deploy the endpoint endpoint\n",
    "predictor = huggingface_model.deploy(\n",
    "    initial_instance_count=1,      # number of instances\n",
    "    instance_type=\"ml.inf2.xlarge\" # AWS Inferentia Instance\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c858560",
   "metadata": {},
   "source": [
    "# 5. Run and evaluate Inference performance of BERT on Inferentia\n",
    "\n",
    "TheÂ `.deploy()`Â returns anÂ `HuggingFacePredictor`Â object which can be used to request inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2ff049",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "  \"inputs\": \"the mesmerizing performances of the leads keep the film grounded and keep the audience riveted .\",\n",
    "}\n",
    "\n",
    "res = predictor.predict(data=data)\n",
    "res"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0a146346",
   "metadata": {},
   "source": [
    "We managed to deploy our neuron compiled BERT to AWS Inferentia on Amazon SageMaker. Now, let's test its performance of it. As a dummy load test will we use threading to send 10000 requests to our endpoint with 10 threads.\n",
    "\n",
    "_Note: When running the load test we environment was based in europe and the endpoint is deployed in us-east-2._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237f198c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "\n",
    "number_of_threads = 10\n",
    "number_of_requests = int(10000 // number_of_threads)\n",
    "print(f\"number of threads: {number_of_threads}\")\n",
    "print(f\"number of requests per thread: {number_of_requests}\")\n",
    "\n",
    "def send_rquests():\n",
    "    for _ in range(number_of_requests):\n",
    "        predictor.predict(data={\"inputs\": \"it 's a charming and often affecting journey .\"})\n",
    "    print(\"done\")\n",
    "\n",
    "# Create multiple threads\n",
    "threads = [threading.Thread(target=send_rquests) for _ in range(number_of_threads) ]\n",
    "# start all threads\n",
    "[t.start() for t in threads]\n",
    "# wait for all threads to finish\n",
    "[t.join() for t in threads]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b0b8e25d",
   "metadata": {},
   "source": [
    "Sending 10000 requests with 10 threads takes around 86 seconds. This means we can run around ~116 inferences per second. But keep in mind that includes the network latency from europe to us-east-2. \n",
    "When we inspect the latency of the endpoint through cloudwatch we can see that the average latency is around 4ms. This means we can run around 500 inferences per second, without network overhead or framework overhead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4d916b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"https://console.aws.amazon.com/cloudwatch/home?region={sess.boto_region_name}#metricsV2:graph=~(metrics~(~(~'AWS*2fSageMaker~'ModelLatency~'EndpointName~'{predictor.endpoint_name}~'VariantName~'AllTraffic))~view~'timeSeries~stacked~false~region~'{sess.boto_region_name}~start~'-PT5M~end~'P0D~stat~'Average~period~30);query=~'*7bAWS*2fSageMaker*2cEndpointName*2cVariantName*7d*20{predictor.endpoint_name}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "af0f26d0",
   "metadata": {},
   "source": [
    "The average latency for our BERT model is `3.8-4.1ms` for a sequence length of 128.  \n",
    "\n",
    "![performance](./imgs/performance.png)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1030c87f",
   "metadata": {},
   "source": [
    "### Delete model and endpoint\n",
    "\n",
    "To clean up, we can delete the model and endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8917d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_model()\n",
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b656e81d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c281c456f1b8161c8906f4af2c08ed2c40c50136979eaae69688b01f70e9f4a9"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
